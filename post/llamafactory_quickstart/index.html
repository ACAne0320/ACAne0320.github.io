<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLaMA-Factory Quickstart | ACAne0320's blog</title><meta name=keywords content><meta name=description content="项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file
项目hugging face地址：
一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。
conda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。
import torch torch.cuda.is_available() 如果返回True，则cuda安装正常。
如果返回False，请检查cuda和pytorch版本是否符合。
使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。
进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。
pip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。"><meta name=author content="ACAne0320"><link rel=canonical href=https://blog.nyaashino.com/post/llamafactory_quickstart/><link crossorigin=anonymous href=/assets/css/stylesheet.50b318546c7d8fb3f21adc93320b0b1cc37f80f9bc37a846b48c24e4bc3dec6c.css integrity rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://blog.nyaashino.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.nyaashino.com/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://blog.nyaashino.com/favicon.ico><link rel=apple-touch-icon href=https://blog.nyaashino.com/favicon.ico><link rel=mask-icon href=https://blog.nyaashino.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="LLaMA-Factory Quickstart"><meta property="og:description" content="项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file
项目hugging face地址：
一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。
conda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。
import torch torch.cuda.is_available() 如果返回True，则cuda安装正常。
如果返回False，请检查cuda和pytorch版本是否符合。
使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。
进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。
pip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.nyaashino.com/post/llamafactory_quickstart/"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-04-01T17:43:46+08:00"><meta property="article:modified_time" content="2024-04-01T17:43:46+08:00"><meta property="og:site_name" content="hugo-PaperMod"><meta name=twitter:card content="summary"><meta name=twitter:title content="LLaMA-Factory Quickstart"><meta name=twitter:description content="项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file
项目hugging face地址：
一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。
conda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。
import torch torch.cuda.is_available() 如果返回True，则cuda安装正常。
如果返回False，请检查cuda和pytorch版本是否符合。
使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。
进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。
pip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.nyaashino.com/post/"},{"@type":"ListItem","position":2,"name":"LLaMA-Factory Quickstart","item":"https://blog.nyaashino.com/post/llamafactory_quickstart/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLaMA-Factory Quickstart","name":"LLaMA-Factory Quickstart","description":"项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file\n项目hugging face地址：\n一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。\nconda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。\nimport torch torch.cuda.is_available() 如果返回True，则cuda安装正常。\n如果返回False，请检查cuda和pytorch版本是否符合。\n使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。\n进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。\npip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。","keywords":[],"articleBody":"项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file\n项目hugging face地址：\n一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。\nconda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。\nimport torch torch.cuda.is_available() 如果返回True，则cuda安装正常。\n如果返回False，请检查cuda和pytorch版本是否符合。\n使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。\n进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。\npip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。\n2、启动图形界面 再在LLaMA-Factory项目目录下使用以下命令启动图形用户界面：\npython src/train_web.py 三、选择模型进行微调 1、模型选择 在LLaMA-Factory中对以下开源LLM模型进行了适配：\n模型名 模型大小 默认模块 Template Baichuan2 7B/13B W_pack baichuan2 BLOOM 560M/1.1B/1.7B/3B/7.1B/176B query_key_value - BLOOMZ 560M/1.1B/1.7B/3B/7.1B/176B query_key_value - ChatGLM3 6B query_key_value chatglm3 DeepSeek (MoE) 7B/16B/67B q_proj,v_proj deepseek Falcon 7B/40B/180B query_key_value falcon Gemma 2B/7B q_proj,v_proj gemma InternLM2 7B/20B wqkv intern2 LLaMA 7B/13B/33B/65B q_proj,v_proj - LLaMA-2 7B/13B/70B q_proj,v_proj llama2 Mistral 7B q_proj,v_proj mistral Mixtral 8x7B q_proj,v_proj mistral OLMo 1B/7B att_proj olmo Phi-1.5/2 1.3B/2.7B q_proj,v_proj - Qwen 1.8B/7B/14B/72B c_attn qwen Qwen1.5 0.5B/1.8B/4B/7B/14B/72B q_proj,v_proj qwen StarCoder2 3B/7B/15B q_proj,v_proj - XVERSE 7B/13B/65B q_proj,v_proj xverse Yi 6B/9B/34B q_proj,v_proj yi Yuan 2B/51B/102B q_proj,v_proj yuan 当然，你也可以使用本地的模型进行微调。只需在模型路径中指定本地模型的文件路径即可。\n2、微调方法选择 full：对整个预训练模型进行微调，包括所有模型参数。 freeze：只使用少部分参数进行训练，把模型的大部分参数冻结。 lora：插入少量参数，只在新插入的参数上进行微调，达到加速效果。冻结预训练模型权重，将可训练的秩分解矩阵注入到Transformer层每个权重中。 3、训练方法选择 方法 全参数训练 部分参数训练 LoRA QLoRA 预训练 ✅ ✅ ✅ ✅ 指令监督微调 ✅ ✅ ✅ ✅ 奖励模型训练 ✅ ✅ ✅ ✅ PPO 训练 ✅ ✅ ✅ ✅ DPO 训练 ✅ ✅ ✅ ✅ ORPO 训练 ✅ ✅ ✅ ✅ 请使用 --quantization_bit 4 参数来启用 QLoRA 训练。\n4、数据集选择 使用方法请参考 data/README_zh.md 文件。\n部分数据集的使用需要确认，我们推荐使用下述命令登录您的 Hugging Face 账户。\npip install --upgrade huggingface_hub huggingface-cli login 自定义数据集 关于数据集文件的格式，请参考 data/README_zh.md 的内容。构建自定义数据集时，既可以使用单个 .json 文件，也可以使用一个数据加载脚本和多个文件。\n使用自定义数据集时，请更新 data/dataset_info.json 文件，该文件的格式请参考 data/README_zh.md。\n四、模型训练 当选择好训练参数后，点击开始即可进行模型训练。\n在图形界面中，可以看到训练进度、日志以及实时显示的损失值变化。\n五、模型推理 当训练完成后，点击刷新适配器，在适配器路径选择刚刚训练好的模型。\n在Chat中点击加载模型。\n输入想要提问的问题，进行测试。\n","wordCount":"243","inLanguage":"en","datePublished":"2024-04-01T17:43:46+08:00","dateModified":"2024-04-01T17:43:46+08:00","author":{"@type":"Person","name":"ACAne0320"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.nyaashino.com/post/llamafactory_quickstart/"},"publisher":{"@type":"Organization","name":"ACAne0320's blog","logo":{"@type":"ImageObject","url":"https://blog.nyaashino.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.nyaashino.com/ accesskey=h title="ACAne0320's blog (Alt + H)">ACAne0320's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.nyaashino.com/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://blog.nyaashino.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.nyaashino.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.nyaashino.com/>Home</a>&nbsp;»&nbsp;<a href=https://blog.nyaashino.com/post/>Posts</a></div><h1 class=post-title>LLaMA-Factory Quickstart</h1><div class=post-meta><span title='2024-04-01 17:43:46 +0800 CST'>April 1, 2024</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;ACAne0320</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%b8%80%e7%8e%af%e5%a2%83%e6%9e%84%e5%bb%ba aria-label=一、环境构建>一、环境构建</a><ul><li><a href=#1%e5%9f%ba%e7%a1%80%e8%99%9a%e6%8b%9f%e7%8e%af%e5%a2%83%e6%9e%84%e5%bb%ba aria-label=1、基础虚拟环境构建>1、基础虚拟环境构建</a></li><li><a href=#2%e5%9c%a8%e8%99%9a%e6%8b%9f%e7%8e%af%e5%a2%83%e4%b8%ad%e5%ae%89%e8%a3%85cudacudnn aria-label=2、在虚拟环境中安装cuda、cudnn>2、在虚拟环境中安装cuda、cudnn</a></li></ul></li><li><a href=#%e4%ba%8c%e5%90%af%e5%8a%a8llama-board-gui aria-label="二、启动LLaMA Board GUI">二、启动LLaMA Board GUI</a><ul><li><a href=#1%e8%ae%be%e7%bd%ae%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f aria-label=1、设置环境变量>1、设置环境变量</a></li><li><a href=#2%e5%90%af%e5%8a%a8%e5%9b%be%e5%bd%a2%e7%95%8c%e9%9d%a2 aria-label=2、启动图形界面>2、启动图形界面</a></li></ul></li><li><a href=#%e4%b8%89%e9%80%89%e6%8b%a9%e6%a8%a1%e5%9e%8b%e8%bf%9b%e8%a1%8c%e5%be%ae%e8%b0%83 aria-label=三、选择模型进行微调>三、选择模型进行微调</a><ul><li><a href=#1%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9 aria-label=1、模型选择>1、模型选择</a></li><li><a href=#2%e5%be%ae%e8%b0%83%e6%96%b9%e6%b3%95%e9%80%89%e6%8b%a9 aria-label=2、微调方法选择>2、微调方法选择</a></li><li><a href=#3%e8%ae%ad%e7%bb%83%e6%96%b9%e6%b3%95%e9%80%89%e6%8b%a9 aria-label=3、训练方法选择>3、训练方法选择</a></li><li><a href=#4%e6%95%b0%e6%8d%ae%e9%9b%86%e9%80%89%e6%8b%a9 aria-label=4、数据集选择>4、数据集选择</a><ul><li><a href=#%e8%87%aa%e5%ae%9a%e4%b9%89%e6%95%b0%e6%8d%ae%e9%9b%86 aria-label=自定义数据集>自定义数据集</a></li></ul></li></ul></li><li><a href=#%e5%9b%9b%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83 aria-label=四、模型训练>四、模型训练</a></li><li><a href=#%e4%ba%94%e6%a8%a1%e5%9e%8b%e6%8e%a8%e7%90%86 aria-label=五、模型推理>五、模型推理</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file</p><p>项目hugging face地址：</p><h2 id=一环境构建>一、环境构建<a hidden class=anchor aria-hidden=true href=#一环境构建>#</a></h2><h3 id=1基础虚拟环境构建>1、基础虚拟环境构建<a hidden class=anchor aria-hidden=true href=#1基础虚拟环境构建>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>git clone https:<span style=color:#f92672>//</span>github<span style=color:#f92672>.</span>com<span style=color:#f92672>/</span>hiyouga<span style=color:#f92672>/</span>LLaMA<span style=color:#f92672>-</span>Factory<span style=color:#f92672>.</span>git
</span></span><span style=display:flex><span>conda create <span style=color:#f92672>-</span>n llama_factory python<span style=color:#f92672>=</span><span style=color:#ae81ff>3.10</span>
</span></span><span style=display:flex><span>conda activate llama_factory
</span></span><span style=display:flex><span>cd LLaMA<span style=color:#f92672>-</span>Factory
</span></span><span style=display:flex><span>pip install <span style=color:#f92672>-</span>r requirements<span style=color:#f92672>.</span>txt
</span></span></code></pre></div><p>如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 <code>bitsandbytes</code> 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的<a href=https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels>发布版本</a>。</p><pre tabindex=0><code>pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
</code></pre><h3 id=2在虚拟环境中安装cudacudnn>2、在虚拟环境中安装cuda、cudnn<a hidden class=anchor aria-hidden=true href=#2在虚拟环境中安装cudacudnn>#</a></h3><p>在新建的虚拟环境中使用conda安装cuda和cudnn。</p><pre tabindex=0><code>conda install cudatoolkit cudnn
</code></pre><p>安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available()
</span></span></code></pre></div><p>如果返回True，则cuda安装正常。</p><p>如果返回False，请检查cuda和pytorch版本是否符合。</p><blockquote><p>使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用<code>conda list</code>命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。</p></blockquote><p>进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。</p><pre tabindex=0><code>pip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118
</code></pre><h2 id=二启动llama-board-gui>二、启动LLaMA Board GUI<a hidden class=anchor aria-hidden=true href=#二启动llama-board-gui>#</a></h2><h3 id=1设置环境变量>1、设置环境变量<a hidden class=anchor aria-hidden=true href=#1设置环境变量>#</a></h3><pre tabindex=0><code>CUDA_VISIBLE_DEVICES=0 python src/train_web.py
</code></pre><p>在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。</p><p><img loading=lazy src=/LLaMA-Factory-Quickstart.assets/image-20240401163412410.png alt=image-20240401163412410></p><h3 id=2启动图形界面>2、启动图形界面<a hidden class=anchor aria-hidden=true href=#2启动图形界面>#</a></h3><p>再在LLaMA-Factory项目目录下使用以下命令启动图形用户界面：</p><pre tabindex=0><code>python src/train_web.py
</code></pre><p><img loading=lazy src=/LLaMA-Factory-Quickstart.assets/image-20240401163659575.png alt=image-20240401163659575></p><h2 id=三选择模型进行微调>三、选择模型进行微调<a hidden class=anchor aria-hidden=true href=#三选择模型进行微调>#</a></h2><h3 id=1模型选择>1、模型选择<a hidden class=anchor aria-hidden=true href=#1模型选择>#</a></h3><p>在LLaMA-Factory中对以下开源LLM模型进行了适配：</p><table><thead><tr><th>模型名</th><th>模型大小</th><th>默认模块</th><th>Template</th></tr></thead><tbody><tr><td><a href=https://huggingface.co/baichuan-inc>Baichuan2</a></td><td>7B/13B</td><td>W_pack</td><td>baichuan2</td></tr><tr><td><a href=https://huggingface.co/bigscience/bloom>BLOOM</a></td><td>560M/1.1B/1.7B/3B/7.1B/176B</td><td>query_key_value</td><td>-</td></tr><tr><td><a href=https://huggingface.co/bigscience/bloomz>BLOOMZ</a></td><td>560M/1.1B/1.7B/3B/7.1B/176B</td><td>query_key_value</td><td>-</td></tr><tr><td><a href=https://huggingface.co/THUDM/chatglm3-6b>ChatGLM3</a></td><td>6B</td><td>query_key_value</td><td>chatglm3</td></tr><tr><td><a href=https://huggingface.co/deepseek-ai>DeepSeek (MoE)</a></td><td>7B/16B/67B</td><td>q_proj,v_proj</td><td>deepseek</td></tr><tr><td><a href=https://huggingface.co/tiiuae>Falcon</a></td><td>7B/40B/180B</td><td>query_key_value</td><td>falcon</td></tr><tr><td><a href=https://huggingface.co/google>Gemma</a></td><td>2B/7B</td><td>q_proj,v_proj</td><td>gemma</td></tr><tr><td><a href=https://huggingface.co/internlm>InternLM2</a></td><td>7B/20B</td><td>wqkv</td><td>intern2</td></tr><tr><td><a href=https://github.com/facebookresearch/llama>LLaMA</a></td><td>7B/13B/33B/65B</td><td>q_proj,v_proj</td><td>-</td></tr><tr><td><a href=https://huggingface.co/meta-llama>LLaMA-2</a></td><td>7B/13B/70B</td><td>q_proj,v_proj</td><td>llama2</td></tr><tr><td><a href=https://huggingface.co/mistralai>Mistral</a></td><td>7B</td><td>q_proj,v_proj</td><td>mistral</td></tr><tr><td><a href=https://huggingface.co/mistralai>Mixtral</a></td><td>8x7B</td><td>q_proj,v_proj</td><td>mistral</td></tr><tr><td><a href=https://huggingface.co/allenai>OLMo</a></td><td>1B/7B</td><td>att_proj</td><td>olmo</td></tr><tr><td><a href=https://huggingface.co/microsoft>Phi-1.5/2</a></td><td>1.3B/2.7B</td><td>q_proj,v_proj</td><td>-</td></tr><tr><td><a href=https://huggingface.co/Qwen>Qwen</a></td><td>1.8B/7B/14B/72B</td><td>c_attn</td><td>qwen</td></tr><tr><td><a href=https://huggingface.co/Qwen>Qwen1.5</a></td><td>0.5B/1.8B/4B/7B/14B/72B</td><td>q_proj,v_proj</td><td>qwen</td></tr><tr><td><a href=https://huggingface.co/bigcode>StarCoder2</a></td><td>3B/7B/15B</td><td>q_proj,v_proj</td><td>-</td></tr><tr><td><a href=https://huggingface.co/xverse>XVERSE</a></td><td>7B/13B/65B</td><td>q_proj,v_proj</td><td>xverse</td></tr><tr><td><a href=https://huggingface.co/01-ai>Yi</a></td><td>6B/9B/34B</td><td>q_proj,v_proj</td><td>yi</td></tr><tr><td><a href=https://huggingface.co/IEITYuan>Yuan</a></td><td>2B/51B/102B</td><td>q_proj,v_proj</td><td>yuan</td></tr></tbody></table><p>当然，你也可以使用本地的模型进行微调。只需在模型路径中指定本地模型的文件路径即可。</p><p><img loading=lazy src=/LLaMA-Factory-Quickstart.assets/image-20240401164408815.png alt=image-20240401164408815></p><h3 id=2微调方法选择>2、微调方法选择<a hidden class=anchor aria-hidden=true href=#2微调方法选择>#</a></h3><ul><li>full：对整个预训练模型进行微调，包括所有模型参数。</li><li>freeze：只使用少部分参数进行训练，把模型的大部分参数冻结。</li><li>lora：插入少量参数，只在新插入的参数上进行微调，达到加速效果。冻结预训练模型权重，将可训练的秩分解矩阵注入到Transformer层每个权重中。</li></ul><h3 id=3训练方法选择>3、训练方法选择<a hidden class=anchor aria-hidden=true href=#3训练方法选择>#</a></h3><table><thead><tr><th>方法</th><th>全参数训练</th><th>部分参数训练</th><th>LoRA</th><th>QLoRA</th></tr></thead><tbody><tr><td>预训练</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>指令监督微调</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>奖励模型训练</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>PPO 训练</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>DPO 训练</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>ORPO 训练</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr></tbody></table><blockquote><p>请使用 <code>--quantization_bit 4</code> 参数来启用 QLoRA 训练。</p></blockquote><h3 id=4数据集选择>4、数据集选择<a hidden class=anchor aria-hidden=true href=#4数据集选择>#</a></h3><p>使用方法请参考 <a href=https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md>data/README_zh.md</a> 文件。</p><p>部分数据集的使用需要确认，我们推荐使用下述命令登录您的 Hugging Face 账户。</p><pre tabindex=0><code>pip install --upgrade huggingface_hub
huggingface-cli login
</code></pre><h4 id=自定义数据集>自定义数据集<a hidden class=anchor aria-hidden=true href=#自定义数据集>#</a></h4><p>关于数据集文件的格式，请参考 <a href=https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md>data/README_zh.md</a> 的内容。构建自定义数据集时，既可以使用单个 <code>.json</code> 文件，也可以使用一个<a href=https://huggingface.co/docs/datasets/dataset_script>数据加载脚本</a>和多个文件。</p><p>使用自定义数据集时，请更新 <code>data/dataset_info.json</code> 文件，该文件的格式请参考 <code>data/README_zh.md</code>。</p><h2 id=四模型训练>四、模型训练<a hidden class=anchor aria-hidden=true href=#四模型训练>#</a></h2><p>当选择好训练参数后，点击<code>开始</code>即可进行模型训练。</p><p>在图形界面中，可以看到训练进度、日志以及实时显示的损失值变化。</p><p><img loading=lazy src=/LLaMA-Factory-Quickstart.assets/image-20240401170955713.png alt=image-20240401170955713></p><h2 id=五模型推理>五、模型推理<a hidden class=anchor aria-hidden=true href=#五模型推理>#</a></h2><p>当训练完成后，点击<code>刷新适配器</code>，在适配器路径选择刚刚训练好的模型。</p><p><img loading=lazy src=/LLaMA-Factory-Quickstart.assets/image-20240401171545833.png alt=image-20240401171545833></p><p>在Chat中点击加载模型。</p><p><img loading=lazy src=/LLaMA-Factory-Quickstart.assets/image-20240401171701057.png alt=image-20240401171701057></p><p>输入想要提问的问题，进行测试。</p><p><img loading=lazy src=/LLaMA-Factory-Quickstart.assets/image-20240401171831891.png alt=image-20240401171831891></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://blog.nyaashino.com/post/2024_review/><span class=title>« Prev</span><br><span>Do it for youself</span></a>
<a class=next href=https://blog.nyaashino.com/post/new_begining/><span class=title>Next »</span><br><span>New Beginning!</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://blog.nyaashino.com/>ACAne0320's blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>