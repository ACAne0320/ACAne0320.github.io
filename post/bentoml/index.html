<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>BentoML使用流程 | ACAne0320&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="BentoML 基本流程：
train.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。
bentoml.(framename).save_model(modelname, model)
framename取决于你构建机器学习模型时使用的框架。
modelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。
model为你所构建的模型的变量名
假如我此前已经定义了两个变量：
mnist_clf = &#39;tensorflow_mnist&#39;
mnist_model = tf.keras.models.load_model(&#39;models/mnist_model.h5&#39;)
那么有以下保存模型的例子：
bentoml.tensorflow.save_model(mnist_clf, mnist_model)
e.g. 基于sklearn实现的iris数据集模型：
import bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=&#39;scale&#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.">
<meta name="author" content="">
<link rel="canonical" href="https://ACAne0320.github.io/post/bentoml/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5c25c975546c048d1a5600aadb48425ae1bc921a9a18fe67d6955c9535260811.css" integrity="" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://ACAne0320.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ACAne0320.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ACAne0320.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ACAne0320.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ACAne0320.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="BentoML使用流程" />
<meta property="og:description" content="BentoML 基本流程：
train.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。
bentoml.(framename).save_model(modelname, model)
framename取决于你构建机器学习模型时使用的框架。
modelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。
model为你所构建的模型的变量名
假如我此前已经定义了两个变量：
mnist_clf = &#39;tensorflow_mnist&#39;
mnist_model = tf.keras.models.load_model(&#39;models/mnist_model.h5&#39;)
那么有以下保存模型的例子：
bentoml.tensorflow.save_model(mnist_clf, mnist_model)
e.g. 基于sklearn实现的iris数据集模型：
import bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=&#39;scale&#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ACAne0320.github.io/post/bentoml/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-10-26T17:20:02+08:00" />
<meta property="article:modified_time" content="2023-10-26T17:20:02+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="BentoML使用流程"/>
<meta name="twitter:description" content="BentoML 基本流程：
train.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。
bentoml.(framename).save_model(modelname, model)
framename取决于你构建机器学习模型时使用的框架。
modelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。
model为你所构建的模型的变量名
假如我此前已经定义了两个变量：
mnist_clf = &#39;tensorflow_mnist&#39;
mnist_model = tf.keras.models.load_model(&#39;models/mnist_model.h5&#39;)
那么有以下保存模型的例子：
bentoml.tensorflow.save_model(mnist_clf, mnist_model)
e.g. 基于sklearn实现的iris数据集模型：
import bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=&#39;scale&#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://ACAne0320.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "BentoML使用流程",
      "item": "https://ACAne0320.github.io/post/bentoml/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "BentoML使用流程",
  "name": "BentoML使用流程",
  "description": "BentoML 基本流程：\ntrain.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。\nbentoml.(framename).save_model(modelname, model)\nframename取决于你构建机器学习模型时使用的框架。\nmodelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。\nmodel为你所构建的模型的变量名\n假如我此前已经定义了两个变量：\nmnist_clf = 'tensorflow_mnist'\nmnist_model = tf.keras.models.load_model('models/mnist_model.h5')\n那么有以下保存模型的例子：\nbentoml.tensorflow.save_model(mnist_clf, mnist_model)\ne.g. 基于sklearn实现的iris数据集模型：\nimport bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=\u0026#39;scale\u0026#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.",
  "keywords": [
    
  ],
  "articleBody": "BentoML 基本流程：\ntrain.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。\nbentoml.(framename).save_model(modelname, model)\nframename取决于你构建机器学习模型时使用的框架。\nmodelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。\nmodel为你所构建的模型的变量名\n假如我此前已经定义了两个变量：\nmnist_clf = 'tensorflow_mnist'\nmnist_model = tf.keras.models.load_model('models/mnist_model.h5')\n那么有以下保存模型的例子：\nbentoml.tensorflow.save_model(mnist_clf, mnist_model)\ne.g. 基于sklearn实现的iris数据集模型：\nimport bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma='scale') clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.sklearn.save_model(\"iris_clf\", clf) print(f\"Model saved: {saved_model}\") # Model saved: Model(tag=\"iris_clf:zy3dfgxzqkjrlgxi\") 以下是BentoML目前支持的机器学习框架：\nCatBoost Diffusers fast.ai Keras LightGBM MLflow ONNX PyTorch PyTorch Lightning Scikit-Learn TensorFlow Transformers XGBoost Detectron2 EasyORC 二、创建服务 创建一个service.py文件以提供服务：\n# service.py import numpy as np from PIL.Image import Image as PILImage import bentoml from bentoml.io import Image from bentoml.io import NumpyNdarray # 实例化runner对象 mnist_runner = bentoml.tensorflow.get(\"tensorflow_mnist:latest\").to_runner() # 创建服务 svc = bentoml.Service( name=\"tensorflow_mnist_demo\", runners=[mnist_runner], ) # 提供预测服务的函数 @svc.api(input=Image(), output=NumpyNdarray(dtype=\"float32\")) async def predict_image(f: PILImage) -\u003e \"np.ndarray\": assert isinstance(f, PILImage) arr = np.array(f) / 255.0 assert arr.shape == (28, 28) # We are using greyscale image and our PyTorch model expect one # extra channel dimension arr = np.expand_dims(arr, (0, 3)).astype(\"float32\") # reshape to [1, 28, 28, 1] return await mnist_runner.async_run(arr) 现在，我们有了一个可以对MNIST手写数字识别数据集中的图片进行预测的BentoML服务了。\n三、构建Bento 一旦定义了服务，我们就可以将模型和服务制作成bento，Bento是服务的发布格式，它是一个独立的归档文件，运行服务所需的所有源代码、模型文件和依赖关系规范。\n要构建Bento，首先要在项目目录中创建一个bentofile.yaml文件：\nservice: \"service:svc\" description: \"file: ./README.md\" labels: owner: bentoml-team stage: demo include: - \"*.py\" exclude: - \"locustfile.py\" python: lock_packages: false packages: - tensorflow - Pillow BentoML在bentofile.yaml中提供了大量构建选项，用于自定义Python依赖关系、cuda安装、docker镜像分发等等。更多有关bentofile.yaml选项的信息请点击构建Bentos。\n接下来就可以在包含service.py和bentofile.yaml文件的目录下构建bento了！\n运行bentoml build CLI 命令：\n$ bentoml build ██████╗ ███████╗███╗ ██╗████████╗ ██████╗ ███╗ ███╗██╗ ██╔══██╗██╔════╝████╗ ██║╚══██╔══╝██╔═══██╗████╗ ████║██║ ██████╔╝█████╗ ██╔██╗ ██║ ██║ ██║ ██║██╔████╔██║██║ ██╔══██╗██╔══╝ ██║╚██╗██║ ██║ ██║ ██║██║╚██╔╝██║██║ ██████╔╝███████╗██║ ╚████║ ██║ ╚██████╔╝██║ ╚═╝ ██║███████╗ ╚═════╝ ╚══════╝╚═╝ ╚═══╝ ╚═╝ ╚═════╝ ╚═╝ ╚═╝╚══════╝ Successfully built Bento(tag=\"tensorflow_mnist_demo:n5g45ibme2efgedi\"). Possible next steps: * Containerize your Bento with `bentoml containerize`: $ bentoml containerize tensorflow_mnist_demo:n5g45ibme2efgedi [or bentoml build --containerize] * Push to BentoCloud with `bentoml push`: $ bentoml push tensorflow_mnist_demo:n5g45ibme2efgedi [or bentoml build --push] 当然 你也可以指定需要构建的bento：\nbentoml build -f ./src/my_project_a/bento_fraud_detect.yaml ./src/\n和保存模型类似，新创建的bento也会自动生成唯一的版本标签。\n你可以使用bentoml list查看你在本地构建的所有bento\n当你有多个bento的时候，你是否需要清理一些不需要的bento？\n你可以使用bentoml delete {bentoname:version}来删除不需要的bento\n四、进行预测 1、提供服务 目前已知的有两种提供服务的方式：\n本地使用命令行提供服务 ​\t首先使用bentoml serve {bentoname:version} CLI命令来运行它，得到一个bentoserver：\n​\t此时我们向localhost:3000/{svc.api_name}发送请求时，携带我们需要让其进行预测的数据以及数据类型，便可以获得相应的预测结果了。\n将模型推送到Yatai上并部署 ​\t在bento build完成之后，可以将模型push到yatai上，在Yatai上根据提示部署模型。设置相应的服务器端口便可以使用预测服务了。\n2、进行预测 目前已知的有三种进行预测的方式：\n不使用服务，直接加载本地使用bentoml.{framename}.save_model方式构建出来的模型，再进行预测（目前eb使用的就是这种方式 import bentoml tensorflow_mnist_runner = bentoml.tensorflow.get(\"tensorflow_mnist:latest\").to_runner() tensorflow_mnist_runner.init_local() tensorflow_mnist_runner.run(image) 使用服务，发送请求，通过服务进行预测从而获取结果 import requests requests_url = \"http://127.0.0.1:3000/predict_image\" def predict(img_url, request_url): # 获取二进制的url with open(img_url, 'rb') as f: img_bytes = f.read() # 向创建的服务发送预测请求 包含 请求地址、请求头（内容类型）、以及预测的数据 result = requests.post( \"http://127.0.0.1:3000/predict_image\", headers={\"content-type\": \"image/png\"}, data=img_bytes, ).text # 将预测结果转化为对应标签 result = eval(result)[0] max_value = max(result) max_index = result.index(max_value) # 返回预测出来的标签 return max_index results = [] for i in range(10): result = predict(f'./samples/{i}.png', requests_url) results.append(result) print(results) \u003e\u003e output:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 使用Swagger UI，交互式的发送预测请求 点击Try it out:\n选择图片发送请求：\n查看返回结果：\n五、Swagger UI 基本流程如下：\n在构建Bento的时候在bento_name/version/apis/文件夹下生成了openapi.yaml文件，生成的openapi.yaml文件它遵循以下规范。 当使用bentoml serve在本地启动了BentoML服务或者在Yatai上部署后，它便会将openapi.yaml文档自动提供给Swagger UI，使其可以在前端渲染。 其中的docs.json是由Swagger UI从BentoML服务的OpenAPI规范文件自动生成的。 Swagger UI 中自动生成的内容\nsrc/bentoml/_internal/service/openapi/__init__.py\nopenapi.yaml文件生成的代码路径\nsrc/bentoml/_internal/bento/bento.py\n基于__init__.py中的函数openapi_spec构建\n定义了服务上传多个文件输入/输出的 API 规范\nsrc/bentoml/_internal/io_descriptors/multipart.py\n定义端点和前端内容\nsrc/bentoml/_internal/server/http_app.py\n",
  "wordCount" : "436",
  "inLanguage": "en",
  "datePublished": "2023-10-26T17:20:02+08:00",
  "dateModified": "2023-10-26T17:20:02+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ACAne0320.github.io/post/bentoml/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ACAne0320's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ACAne0320.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ACAne0320.github.io" accesskey="h" title="ACAne0320&#39;s blog (Alt + H)">ACAne0320&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      BentoML使用流程
    </h1>
    <div class="post-meta"><span title='2023-10-26 17:20:02 +0800 CST'>October 26, 2023</span>

</div>
  </header> 
  <div class="post-content"><h2 id="bentoml">BentoML<a hidden class="anchor" aria-hidden="true" href="#bentoml">#</a></h2>
<p>基本流程：</p>
<ol>
<li><code>train.py</code>训练模型</li>
<li>使用bentoml保存模型</li>
<li>构建<code>service.py</code>，创建相应的服务</li>
<li>创建<code>bentofile.yaml</code>文件</li>
<li>使用命令行<code>bentoml build</code> 构建Bento</li>
<li>将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务</li>
<li>发送预测请求</li>
</ol>
<h3 id="一使用bentoml保存模型">一、使用BentoML保存模型<a hidden class="anchor" aria-hidden="true" href="#一使用bentoml保存模型">#</a></h3>
<p>如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。</p>
<p><code>bentoml.(framename).save_model(modelname, model)</code></p>
<blockquote>
<p>framename取决于你构建机器学习模型时使用的框架。</p>
<p>modelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。</p>
<p>model为你所构建的模型的变量名</p>
</blockquote>
<p>假如我此前已经定义了两个变量：</p>
<p><code>mnist_clf = 'tensorflow_mnist'</code></p>
<p><code>mnist_model = tf.keras.models.load_model('models/mnist_model.h5')</code></p>
<p>那么有以下保存模型的例子：</p>
<p><code>bentoml.tensorflow.save_model(mnist_clf, mnist_model)</code></p>
<p>e.g. 基于sklearn实现的iris数据集模型：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> bentoml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> svm
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> datasets
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load training data set</span>
</span></span><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>load_iris()
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data, iris<span style="color:#f92672">.</span>target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train the model</span>
</span></span><span style="display:flex;"><span>clf <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>SVC(gamma<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;scale&#39;</span>)
</span></span><span style="display:flex;"><span>clf<span style="color:#f92672">.</span>fit(X, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save model to the BentoML local model store</span>
</span></span><span style="display:flex;"><span>saved_model <span style="color:#f92672">=</span> bentoml<span style="color:#f92672">.</span>sklearn<span style="color:#f92672">.</span>save_model(<span style="color:#e6db74">&#34;iris_clf&#34;</span>, clf)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Model saved: </span><span style="color:#e6db74">{</span>saved_model<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Model saved: Model(tag=&#34;iris_clf:zy3dfgxzqkjrlgxi&#34;)</span>
</span></span></code></pre></div><p>以下是BentoML目前支持的机器学习框架：</p>
<ul>
<li>CatBoost</li>
<li>Diffusers</li>
<li>fast.ai</li>
<li>Keras</li>
<li>LightGBM</li>
<li>MLflow</li>
<li>ONNX</li>
<li>PyTorch</li>
<li>PyTorch Lightning</li>
<li>Scikit-Learn</li>
<li>TensorFlow</li>
<li>Transformers</li>
<li>XGBoost</li>
<li>Detectron2</li>
<li>EasyORC</li>
</ul>
<h3 id="二创建服务">二、创建服务<a hidden class="anchor" aria-hidden="true" href="#二创建服务">#</a></h3>
<p>创建一个<code>service.py</code>文件以提供服务：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># service.py</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> PIL.Image <span style="color:#f92672">import</span> Image <span style="color:#66d9ef">as</span> PILImage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> bentoml
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bentoml.io <span style="color:#f92672">import</span> Image
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bentoml.io <span style="color:#f92672">import</span> NumpyNdarray
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 实例化runner对象</span>
</span></span><span style="display:flex;"><span>mnist_runner <span style="color:#f92672">=</span> bentoml<span style="color:#f92672">.</span>tensorflow<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;tensorflow_mnist:latest&#34;</span>)<span style="color:#f92672">.</span>to_runner()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建服务</span>
</span></span><span style="display:flex;"><span>svc <span style="color:#f92672">=</span> bentoml<span style="color:#f92672">.</span>Service(
</span></span><span style="display:flex;"><span>    name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tensorflow_mnist_demo&#34;</span>,
</span></span><span style="display:flex;"><span>    runners<span style="color:#f92672">=</span>[mnist_runner],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 提供预测服务的函数</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@svc.api</span>(input<span style="color:#f92672">=</span>Image(), output<span style="color:#f92672">=</span>NumpyNdarray(dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;float32&#34;</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_image</span>(f: PILImage) <span style="color:#f92672">-&gt;</span> <span style="color:#e6db74">&#34;np.ndarray&#34;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> isinstance(f, PILImage)
</span></span><span style="display:flex;"><span>    arr <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(f) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> arr<span style="color:#f92672">.</span>shape <span style="color:#f92672">==</span> (<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># We are using greyscale image and our PyTorch model expect one</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># extra channel dimension</span>
</span></span><span style="display:flex;"><span>    arr <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>expand_dims(arr, (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>))<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#34;float32&#34;</span>)  <span style="color:#75715e"># reshape to [1, 28, 28, 1]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">await</span> mnist_runner<span style="color:#f92672">.</span>async_run(arr)
</span></span></code></pre></div><p>现在，我们有了一个可以对MNIST手写数字识别数据集中的图片进行预测的BentoML服务了。</p>
<h3 id="三构建bento">三、构建Bento<a hidden class="anchor" aria-hidden="true" href="#三构建bento">#</a></h3>
<p>一旦定义了服务，我们就可以将模型和服务制作成<code>bento</code>，Bento是服务的发布格式，它是一个独立的归档文件，运行服务所需的所有源代码、模型文件和依赖关系规范。</p>
<p>要构建Bento，首先要在项目目录中创建一个bentofile.yaml文件：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">service</span>: <span style="color:#e6db74">&#34;service:svc&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">description</span>: <span style="color:#e6db74">&#34;file: ./README.md&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">owner</span>: <span style="color:#ae81ff">bentoml-team</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">stage</span>: <span style="color:#ae81ff">demo</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">include</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#e6db74">&#34;*.py&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">exclude</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#e6db74">&#34;locustfile.py&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">python</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">lock_packages</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">packages</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">tensorflow</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">Pillow</span>
</span></span></code></pre></div><p>BentoML在bentofile.yaml中提供了大量构建选项，用于自定义Python依赖关系、cuda安装、docker镜像分发等等。更多有关bentofile.yaml选项的信息请点击<a href="https://docs.bentoml.org/en/latest/concepts/bento.html">构建Bentos</a>。</p>
<p>接下来就可以在包含<code>service.py</code>和<code>bentofile.yaml</code>文件的目录下构建bento了！</p>
<p>运行<code>bentoml build</code> CLI 命令：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ bentoml build
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>██████╗ ███████╗███╗   ██╗████████╗ ██████╗ ███╗   ███╗██╗
</span></span><span style="display:flex;"><span>██╔══██╗██╔════╝████╗  ██║╚══██╔══╝██╔═══██╗████╗ ████║██║
</span></span><span style="display:flex;"><span>██████╔╝█████╗  ██╔██╗ ██║   ██║   ██║   ██║██╔████╔██║██║
</span></span><span style="display:flex;"><span>██╔══██╗██╔══╝  ██║╚██╗██║   ██║   ██║   ██║██║╚██╔╝██║██║
</span></span><span style="display:flex;"><span>██████╔╝███████╗██║ ╚████║   ██║   ╚██████╔╝██║ ╚═╝ ██║███████╗
</span></span><span style="display:flex;"><span>╚═════╝ ╚══════╝╚═╝  ╚═══╝   ╚═╝    ╚═════╝ ╚═╝     ╚═╝╚══════╝
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Successfully built Bento<span style="color:#f92672">(</span>tag<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tensorflow_mnist_demo:n5g45ibme2efgedi&#34;</span><span style="color:#f92672">)</span>.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Possible next steps:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> * Containerize your Bento with <span style="color:#e6db74">`</span>bentoml containerize<span style="color:#e6db74">`</span>:
</span></span><span style="display:flex;"><span>    $ bentoml containerize tensorflow_mnist_demo:n5g45ibme2efgedi  <span style="color:#f92672">[</span>or bentoml build --containerize<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> * Push to BentoCloud with <span style="color:#e6db74">`</span>bentoml push<span style="color:#e6db74">`</span>:
</span></span><span style="display:flex;"><span>    $ bentoml push tensorflow_mnist_demo:n5g45ibme2efgedi <span style="color:#f92672">[</span>or bentoml build --push<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>当然 你也可以指定需要构建的bento：</p>
<p><code>bentoml build -f ./src/my_project_a/bento_fraud_detect.yaml ./src/</code></p>
<p>和<a href="#%E4%B8%80%E3%80%81%E4%BD%BF%E7%94%A8BentoML%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B">保存模型</a>类似，新创建的bento也会自动生成唯一的版本标签。</p>
<p>你可以使用<code>bentoml list</code>查看你在本地构建的所有bento</p>
<p><img loading="lazy" src="./C:/Users/84467/Documents/BentoML.assets/image-20230727104524352.png" alt="image-20230727104524352"  />
</p>
<p>当你有多个bento的时候，你是否需要清理一些不需要的bento？</p>
<p>你可以使用<code>bentoml delete {bentoname:version}</code>来删除不需要的bento</p>
<p><img loading="lazy" src="./C:/Users/84467/Documents/BentoML.assets/image-20230727104829938.png" alt="image-20230727104829938"  />
</p>
<h3 id="四进行预测">四、进行预测<a hidden class="anchor" aria-hidden="true" href="#四进行预测">#</a></h3>
<h4 id="1提供服务">1、提供服务<a hidden class="anchor" aria-hidden="true" href="#1提供服务">#</a></h4>
<p>目前已知的有两种提供服务的方式：</p>
<ol>
<li>本地使用命令行提供服务</li>
</ol>
<p>​	首先使用<code>bentoml serve {bentoname:version}</code> CLI命令来运行它，得到一个bentoserver：</p>
<p><img loading="lazy" src="./C:/Users/84467/Documents/BentoML.assets/image-20230727105503781.png" alt="image-20230727105503781"  />
</p>
<p>​	此时我们向<code>localhost:3000/{svc.api_name}</code>发送请求时，携带我们需要让其进行预测的数据以及数据类型，便可以获得相应的预测结果了。</p>
<ol start="2">
<li>将模型推送到Yatai上并部署</li>
</ol>
<p>​	在bento build完成之后，可以将模型push到yatai上，在Yatai上根据提示部署模型。设置相应的服务器端口便可以使用预测服务了。</p>
<h4 id="2进行预测">2、进行预测<a hidden class="anchor" aria-hidden="true" href="#2进行预测">#</a></h4>
<p>目前已知的有三种进行预测的方式：</p>
<ul>
<li>不使用服务，直接加载本地使用<code>bentoml.{framename}.save_model</code>方式构建出来的模型，再进行预测（目前eb使用的就是这种方式</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> bentoml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensorflow_mnist_runner <span style="color:#f92672">=</span> bentoml<span style="color:#f92672">.</span>tensorflow<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;tensorflow_mnist:latest&#34;</span>)<span style="color:#f92672">.</span>to_runner()
</span></span><span style="display:flex;"><span>tensorflow_mnist_runner<span style="color:#f92672">.</span>init_local()
</span></span><span style="display:flex;"><span>tensorflow_mnist_runner<span style="color:#f92672">.</span>run(image)
</span></span></code></pre></div><ul>
<li>使用服务，发送请求，通过服务进行预测从而获取结果</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>requests_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://127.0.0.1:3000/predict_image&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(img_url, request_url):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 获取二进制的url</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(img_url, <span style="color:#e6db74">&#39;rb&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>        img_bytes <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 向创建的服务发送预测请求 包含 请求地址、请求头（内容类型）、以及预测的数据</span>
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>post(
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;http://127.0.0.1:3000/predict_image&#34;</span>,
</span></span><span style="display:flex;"><span>      headers<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;content-type&#34;</span>: <span style="color:#e6db74">&#34;image/png&#34;</span>},
</span></span><span style="display:flex;"><span>      data<span style="color:#f92672">=</span>img_bytes,
</span></span><span style="display:flex;"><span>    )<span style="color:#f92672">.</span>text
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 将预测结果转化为对应标签</span>
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> eval(result)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    max_value <span style="color:#f92672">=</span> max(result)
</span></span><span style="display:flex;"><span>    max_index <span style="color:#f92672">=</span> result<span style="color:#f92672">.</span>index(max_value)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 返回预测出来的标签</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> max_index
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> predict(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;./samples/</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">.png&#39;</span>, requests_url)
</span></span><span style="display:flex;"><span>    results<span style="color:#f92672">.</span>append(result)
</span></span><span style="display:flex;"><span>print(results)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;</span> output:[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">9</span>]
</span></span></code></pre></div><ul>
<li>使用Swagger UI，交互式的发送预测请求</li>
</ul>
<p>点击Try it out:</p>
<p><img loading="lazy" src="./C:/Users/84467/Documents/BentoML.assets/image-20230727160430809.png" alt="image-20230727160430809"  />
</p>
<p>选择图片发送请求：</p>
<p><img loading="lazy" src="./C:/Users/84467/Documents/BentoML.assets/image-20230727160452338.png" alt="image-20230727160452338"  />
</p>
<p>查看返回结果：</p>
<p><img loading="lazy" src="./C:/Users/84467/Documents/BentoML.assets/image-20230727160515575.png" alt="image-20230727160515575"  />
</p>
<h3 id="五swagger-ui">五、Swagger UI<a hidden class="anchor" aria-hidden="true" href="#五swagger-ui">#</a></h3>
<p>基本流程如下：</p>
<ol>
<li>在构建Bento的时候在<code>bento_name/version/apis/</code>文件夹下生成了<code>openapi.yaml</code>文件，生成的<code>openapi.yaml</code>文件它遵循以下<a href="https://openapi.apifox.cn/">规范</a>。</li>
<li>当使用<code>bentoml serve</code>在本地启动了BentoML服务或者在Yatai上部署后，它便会将<code>openapi.yaml</code>文档自动提供给Swagger UI，使其可以在前端渲染。</li>
<li>其中的<code>docs.json</code>是由Swagger UI从BentoML服务的OpenAPI规范文件自动生成的。</li>
</ol>
<p>Swagger UI 中自动生成的内容</p>
<p><code>src/bentoml/_internal/service/openapi/__init__.py</code></p>
<p>openapi.yaml文件生成的代码路径</p>
<p><code>src/bentoml/_internal/bento/bento.py</code></p>
<p>基于<code>__init__.py</code>中的函数<code>openapi_spec</code>构建</p>
<p>定义了服务上传多个文件输入/输出的 API 规范</p>
<p><code>src/bentoml/_internal/io_descriptors/multipart.py</code></p>
<p>定义端点和前端内容</p>
<p><code>src/bentoml/_internal/server/http_app.py</code></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://ACAne0320.github.io">ACAne0320&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
