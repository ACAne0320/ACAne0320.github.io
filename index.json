[{"content":"","permalink":"https://ACAne0320.github.io/post/test/","summary":"","title":"Test"},{"content":"既然都建好自己的博客了，总得记录些什么吧o.0，于是乎……\n今天是一个值得纪念的日子，为什么捏？请看~\n一、工作 从入职到今天正好四个月，也顺利度过了试用期，工资也勉强够生活（虽然除去必要的开销之后所剩无几了，而且还需要还前几个月一点收入的没有的时候的债，在此特别感谢愿意借我钱的好哥们，虽然你们不一定能读到这篇文章，虽然我还钱还的很慢，但是我会努力的QAQ）。工作氛围也好，压力也没那么大，很幸运遇到这样的公司。\n二、生活 今天搬进新房子啦，是一个一室一厅，虽然小了点，但我还是很喜欢的，租过程很愉快，中介也好，房东也好，都是很有爱的人。可能我的好运都用在这些地方了吧，从得到这份工作开始，最近也很少抱怨生活了，慢慢开始变得积极向上了。很感谢当时对我提供帮助的各位老师们，希望我的表现并没有让你们失望（虽然也不一定能读到这篇文章，我要点名表扬农老师！虽然我当面一直说不出那句感谢😄）。\n三、新的开始 成功转正了，也租了属于自己的一间房子，真正出来工作之后才知道原来生活这么困难，光是供自己活下去都费尽全力了。 所以现在我很敬佩那些靠自己的能力努力活到现在并且保持乐观的人，我也会努力的向他们看齐。\n插个今天搬家的小故事：\n今天搬家的时候叫了一个货拉拉的小面包车，司机也是一个努力、乐观又可爱的人，和我一样也有点内向，但是我能听出来他的很多话都是发自内心的，可能这就是同性相吸吧。出小区的时候，由于停车超过了半个小时，要付停车费，我说我转给你吧，他说不用了，大家都挺不容易的。那一瞬间我有点感动，但也只是笑了笑说这样不好吧，直到最后他也没收我的钱。我也只能回几声感谢，一想到可能再也没有缘分再见了，有点遗憾呢。但是人生就是这样在一段又一段遗憾的中度过的，谁又能一帆风顺呢？\nKeep calm and carry on!\n虽然生活很难，但我们还是要继续前行。\n努力变成乐观又元气满满的样子，谁能想到现在这样的我，在一年以前，是一个门都不敢出的玉玉症+自闭症+社恐+回避型依恋的人呢？很感谢过年的时候一直思考着是否要出去打拼、奋斗的自己，未来的你向你报喜啦，你成功了！虽然现在也很自闭，但是相比以前，已经好太多了，我能看见我自己的进步，所以我并不觉得我比别人差。\n最近加上午休每天也只睡了大概七个小时，睡眠质量极差。\n希望搬到新的环境中会有所改善吧🙏\n4、END 最后送大家正好听到的一首歌的一段歌词：\n只能由我自己来踏出这一步，不能在水边傻傻等待明天到来。\n","permalink":"https://ACAne0320.github.io/post/new_begining/","summary":"既然都建好自己的博客了，总得记录些什么吧o.0，于是乎……\n今天是一个值得纪念的日子，为什么捏？请看~\n一、工作 从入职到今天正好四个月，也顺利度过了试用期，工资也勉强够生活（虽然除去必要的开销之后所剩无几了，而且还需要还前几个月一点收入的没有的时候的债，在此特别感谢愿意借我钱的好哥们，虽然你们不一定能读到这篇文章，虽然我还钱还的很慢，但是我会努力的QAQ）。工作氛围也好，压力也没那么大，很幸运遇到这样的公司。\n二、生活 今天搬进新房子啦，是一个一室一厅，虽然小了点，但我还是很喜欢的，租过程很愉快，中介也好，房东也好，都是很有爱的人。可能我的好运都用在这些地方了吧，从得到这份工作开始，最近也很少抱怨生活了，慢慢开始变得积极向上了。很感谢当时对我提供帮助的各位老师们，希望我的表现并没有让你们失望（虽然也不一定能读到这篇文章，我要点名表扬农老师！虽然我当面一直说不出那句感谢😄）。\n三、新的开始 成功转正了，也租了属于自己的一间房子，真正出来工作之后才知道原来生活这么困难，光是供自己活下去都费尽全力了。 所以现在我很敬佩那些靠自己的能力努力活到现在并且保持乐观的人，我也会努力的向他们看齐。\n插个今天搬家的小故事：\n今天搬家的时候叫了一个货拉拉的小面包车，司机也是一个努力、乐观又可爱的人，和我一样也有点内向，但是我能听出来他的很多话都是发自内心的，可能这就是同性相吸吧。出小区的时候，由于停车超过了半个小时，要付停车费，我说我转给你吧，他说不用了，大家都挺不容易的。那一瞬间我有点感动，但也只是笑了笑说这样不好吧，直到最后他也没收我的钱。我也只能回几声感谢，一想到可能再也没有缘分再见了，有点遗憾呢。但是人生就是这样在一段又一段遗憾的中度过的，谁又能一帆风顺呢？\nKeep calm and carry on!\n虽然生活很难，但我们还是要继续前行。\n努力变成乐观又元气满满的样子，谁能想到现在这样的我，在一年以前，是一个门都不敢出的玉玉症+自闭症+社恐+回避型依恋的人呢？很感谢过年的时候一直思考着是否要出去打拼、奋斗的自己，未来的你向你报喜啦，你成功了！虽然现在也很自闭，但是相比以前，已经好太多了，我能看见我自己的进步，所以我并不觉得我比别人差。\n最近加上午休每天也只睡了大概七个小时，睡眠质量极差。\n希望搬到新的环境中会有所改善吧🙏\n4、END 最后送大家正好听到的一首歌的一段歌词：\n只能由我自己来踏出这一步，不能在水边傻傻等待明天到来。","title":"New Beginning!"},{"content":"BentoML 基本流程：\ntrain.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。\nbentoml.(framename).save_model(modelname, model)\nframename取决于你构建机器学习模型时使用的框架。\nmodelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。\nmodel为你所构建的模型的变量名\n假如我此前已经定义了两个变量：\nmnist_clf = 'tensorflow_mnist'\nmnist_model = tf.keras.models.load_model('models/mnist_model.h5')\n那么有以下保存模型的例子：\nbentoml.tensorflow.save_model(mnist_clf, mnist_model)\ne.g. 基于sklearn实现的iris数据集模型：\nimport bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=\u0026#39;scale\u0026#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.sklearn.save_model(\u0026#34;iris_clf\u0026#34;, clf) print(f\u0026#34;Model saved: {saved_model}\u0026#34;) # Model saved: Model(tag=\u0026#34;iris_clf:zy3dfgxzqkjrlgxi\u0026#34;) 以下是BentoML目前支持的机器学习框架：\nCatBoost Diffusers fast.ai Keras LightGBM MLflow ONNX PyTorch PyTorch Lightning Scikit-Learn TensorFlow Transformers XGBoost Detectron2 EasyORC 二、创建服务 创建一个service.py文件以提供服务：\n# service.py import numpy as np from PIL.Image import Image as PILImage import bentoml from bentoml.io import Image from bentoml.io import NumpyNdarray # 实例化runner对象 mnist_runner = bentoml.tensorflow.get(\u0026#34;tensorflow_mnist:latest\u0026#34;).to_runner() # 创建服务 svc = bentoml.Service( name=\u0026#34;tensorflow_mnist_demo\u0026#34;, runners=[mnist_runner], ) # 提供预测服务的函数 @svc.api(input=Image(), output=NumpyNdarray(dtype=\u0026#34;float32\u0026#34;)) async def predict_image(f: PILImage) -\u0026gt; \u0026#34;np.ndarray\u0026#34;: assert isinstance(f, PILImage) arr = np.array(f) / 255.0 assert arr.shape == (28, 28) # We are using greyscale image and our PyTorch model expect one # extra channel dimension arr = np.expand_dims(arr, (0, 3)).astype(\u0026#34;float32\u0026#34;) # reshape to [1, 28, 28, 1] return await mnist_runner.async_run(arr) 现在，我们有了一个可以对MNIST手写数字识别数据集中的图片进行预测的BentoML服务了。\n三、构建Bento 一旦定义了服务，我们就可以将模型和服务制作成bento，Bento是服务的发布格式，它是一个独立的归档文件，运行服务所需的所有源代码、模型文件和依赖关系规范。\n要构建Bento，首先要在项目目录中创建一个bentofile.yaml文件：\nservice: \u0026#34;service:svc\u0026#34; description: \u0026#34;file: ./README.md\u0026#34; labels: owner: bentoml-team stage: demo include: - \u0026#34;*.py\u0026#34; exclude: - \u0026#34;locustfile.py\u0026#34; python: lock_packages: false packages: - tensorflow - Pillow BentoML在bentofile.yaml中提供了大量构建选项，用于自定义Python依赖关系、cuda安装、docker镜像分发等等。更多有关bentofile.yaml选项的信息请点击构建Bentos。\n接下来就可以在包含service.py和bentofile.yaml文件的目录下构建bento了！\n运行bentoml build CLI 命令：\n$ bentoml build ██████╗ ███████╗███╗ ██╗████████╗ ██████╗ ███╗ ███╗██╗ ██╔══██╗██╔════╝████╗ ██║╚══██╔══╝██╔═══██╗████╗ ████║██║ ██████╔╝█████╗ ██╔██╗ ██║ ██║ ██║ ██║██╔████╔██║██║ ██╔══██╗██╔══╝ ██║╚██╗██║ ██║ ██║ ██║██║╚██╔╝██║██║ ██████╔╝███████╗██║ ╚████║ ██║ ╚██████╔╝██║ ╚═╝ ██║███████╗ ╚═════╝ ╚══════╝╚═╝ ╚═══╝ ╚═╝ ╚═════╝ ╚═╝ ╚═╝╚══════╝ Successfully built Bento(tag=\u0026#34;tensorflow_mnist_demo:n5g45ibme2efgedi\u0026#34;). Possible next steps: * Containerize your Bento with `bentoml containerize`: $ bentoml containerize tensorflow_mnist_demo:n5g45ibme2efgedi [or bentoml build --containerize] * Push to BentoCloud with `bentoml push`: $ bentoml push tensorflow_mnist_demo:n5g45ibme2efgedi [or bentoml build --push] 当然 你也可以指定需要构建的bento：\nbentoml build -f ./src/my_project_a/bento_fraud_detect.yaml ./src/\n和保存模型类似，新创建的bento也会自动生成唯一的版本标签。\n你可以使用bentoml list查看你在本地构建的所有bento\n当你有多个bento的时候，你是否需要清理一些不需要的bento？\n你可以使用bentoml delete {bentoname:version}来删除不需要的bento\n四、进行预测 1、提供服务 目前已知的有两种提供服务的方式：\n本地使用命令行提供服务 ​ 首先使用bentoml serve {bentoname:version} CLI命令来运行它，得到一个bentoserver：\n​ 此时我们向localhost:3000/{svc.api_name}发送请求时，携带我们需要让其进行预测的数据以及数据类型，便可以获得相应的预测结果了。\n将模型推送到Yatai上并部署 ​ 在bento build完成之后，可以将模型push到yatai上，在Yatai上根据提示部署模型。设置相应的服务器端口便可以使用预测服务了。\n2、进行预测 目前已知的有三种进行预测的方式：\n不使用服务，直接加载本地使用bentoml.{framename}.save_model方式构建出来的模型，再进行预测（目前eb使用的就是这种方式 import bentoml tensorflow_mnist_runner = bentoml.tensorflow.get(\u0026#34;tensorflow_mnist:latest\u0026#34;).to_runner() tensorflow_mnist_runner.init_local() tensorflow_mnist_runner.run(image) 使用服务，发送请求，通过服务进行预测从而获取结果 import requests requests_url = \u0026#34;http://127.0.0.1:3000/predict_image\u0026#34; def predict(img_url, request_url): # 获取二进制的url with open(img_url, \u0026#39;rb\u0026#39;) as f: img_bytes = f.read() # 向创建的服务发送预测请求 包含 请求地址、请求头（内容类型）、以及预测的数据 result = requests.post( \u0026#34;http://127.0.0.1:3000/predict_image\u0026#34;, headers={\u0026#34;content-type\u0026#34;: \u0026#34;image/png\u0026#34;}, data=img_bytes, ).text # 将预测结果转化为对应标签 result = eval(result)[0] max_value = max(result) max_index = result.index(max_value) # 返回预测出来的标签 return max_index results = [] for i in range(10): result = predict(f\u0026#39;./samples/{i}.png\u0026#39;, requests_url) results.append(result) print(results) \u0026gt;\u0026gt; output:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 使用Swagger UI，交互式的发送预测请求 点击Try it out:\n选择图片发送请求：\n查看返回结果：\n五、Swagger UI 基本流程如下：\n在构建Bento的时候在bento_name/version/apis/文件夹下生成了openapi.yaml文件，生成的openapi.yaml文件它遵循以下规范。 当使用bentoml serve在本地启动了BentoML服务或者在Yatai上部署后，它便会将openapi.yaml文档自动提供给Swagger UI，使其可以在前端渲染。 其中的docs.json是由Swagger UI从BentoML服务的OpenAPI规范文件自动生成的。 Swagger UI 中自动生成的内容\nsrc/bentoml/_internal/service/openapi/__init__.py\nopenapi.yaml文件生成的代码路径\nsrc/bentoml/_internal/bento/bento.py\n基于__init__.py中的函数openapi_spec构建\n定义了服务上传多个文件输入/输出的 API 规范\nsrc/bentoml/_internal/io_descriptors/multipart.py\n定义端点和前端内容\nsrc/bentoml/_internal/server/http_app.py\n","permalink":"https://ACAne0320.github.io/post/bentoml_quickstart/","summary":"BentoML 基本流程：\ntrain.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。\nbentoml.(framename).save_model(modelname, model)\nframename取决于你构建机器学习模型时使用的框架。\nmodelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。\nmodel为你所构建的模型的变量名\n假如我此前已经定义了两个变量：\nmnist_clf = 'tensorflow_mnist'\nmnist_model = tf.keras.models.load_model('models/mnist_model.h5')\n那么有以下保存模型的例子：\nbentoml.tensorflow.save_model(mnist_clf, mnist_model)\ne.g. 基于sklearn实现的iris数据集模型：\nimport bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=\u0026#39;scale\u0026#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.","title":"BentoML使用流程"}]