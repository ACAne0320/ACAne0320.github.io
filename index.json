[{"content":"饥荒服务器配置 服务器配置 腾讯云轻量级服务器\n2C4G\n系统盘 - SSD云硬盘 70GB\n600GB/月（带宽：6Mbps）\n镜像：Ubuntu22.04-Docker26\n一、下载SteamCMD 1、创建新用户 创建一个名为 steam 的用户帐户以安全地运行 SteamCMD，并将其与操作系统的其余部分隔离。以 root 用户身份登录时请勿运行 steamcmd——这样做会带来安全风险。\n1.以 root 用户身份创建 steam 用户： useradd -m steam or sudo useradd -m steam 2.设置密码 sudo passwd steam 3.切换到steam用户 su - steam 看到 $ 提示符就算切换成功\nubuntu@VM-8-7-ubuntu:~$ su - steam Password: $ $ 你可以通过运行以下命令确认当前用户：\nwhoami 输出应为：\nsteam 你也可以通过以下命令确认当前目录：\npwd 输出应为：\n/home/steam 4.使用管理员账户为 steam 用户赋予 sudo 权限 1.切换回管理员账户（如 ubuntu 用户）：\nexit 2.确认当前用户有 sudo 权限： 输入以下命令检查：\nsudo whoami 如果输出为 root，说明当前用户有权限。\n3.将 steam 用户添加到 sudo 组：\nsudo usermod -aG sudo steam -aG 参数表示将用户添加到一个附加组（这里是 sudo）。\n4.验证 steam 用户的 sudo 权限： 切换回 steam 用户：\nsu - steam 然后测试：\nsudo whoami 如果输出为 root，则权限已成功赋予。\n2、下载SteamCMD 方法1：安装来自软件仓库的软件包 建议从您的分发软件仓库安装 SteamCMD 软件包（如果有）： Ubuntu/Debian\nsudo apt install steamcmd **注意：**如果您使用的是64位计算机，则需要添加 multiverse\nsudo add-apt-repository multiverse sudo dpkg --add-architecture i386 sudo apt update sudo apt install lib32gcc1 steamcmd RedHat/CentOS\nyum install steamcmd Arch Linux\ngit clone https://aur.archlinux.org/steamcmd.git cd steamcmd makepkg -si 链接 steamcmd 可执行文件： ln -s /usr/games/steamcmd steamcmd 方法2：手动安装 在开始之前，您必须先安装运行 SteamCMD 所需的依赖项： Ubuntu/Debian 64-Bit\nsudo apt-get install lib32gcc1 RedHat/CentOS\nyum install glibc libstdc++ RedHat/CentOS 64-Bit\nyum install glibc.i686 libstdc++.i686 以 root 用户身份升级至 steam 用户： su - steam 如果您不是以 root 用户身份登录，而是使用sudo来行使管理权限，请以如下所示来升级至 steam 用户：\nsudo -iu steam 为 SteamCMD 创建目录并切换至该目录。 mkdir ~/Steam \u0026amp;\u0026amp; cd ~/Steam 下载并解压缩适用于 Linux 的 SteamCMD。 curl -sqL \u0026#34;https://steamcdn-a.akamaihd.net/client/installer/steamcmd_linux.tar.gz\u0026#34; | tar zxvf - 中国内地用户可使用以下命令以使用内地节点：\ncurl -sqL \u0026#34;https://media.st.dl.bscstorage.net/client/installer/steamcmd_linux.tar.gz\u0026#34; | tar zxvf - 安装tmux 和/或 screen 以简化服务器管理 sudo apt-get install tmux screen -y; 二、运行 SteamCMD Linux/macOS 打开一个终端并启动 SteamCMD.\n如果您是通过软件仓库中的软件包安装的：\ncd ~ steamcmd 如果您是手动安装的：\ncd ~/Steam ./steamcmd.sh 三、登录 SteamCMD 方法1：匿名登录 下载大多数游戏服务端，你可以选择匿名登录。\nlogin anonymous 方法2：使用 Steam 帐户登录 一些服务端需要你使用一个 Steam 帐户来登录。\n**注意：**为了安全起见建议您创建一个服务端专用的 Steam 帐户。\n**注意：**一个用户同时只能在一处登录（无论是在图形界面客户端还是在 SteamCMD 登录）。\nlogin \u0026lt;用户名\u0026gt; 下一步请输入您的密码。\n如果 Steam 令牌已经在用户帐户上激活，查看你电子邮箱中的 Steam 令牌验证码并输入它。这仅仅只是第一次登录时需要（以及当你删除了 SteamCMD 用于存储登录信息文件的时候）。\n如果你已经成功登录了你的帐户，你应该会看到一条消息。\n四、下载饥荒联机版 打开 SteamCMD 并登录它。\n设置你的应用安装目录（注意：在 Linux/macOS 下使用正斜杠/，在Windows 下使用反斜杠\\）。\nforce_install_dir \u0026lt;路径\u0026gt; 使用以下命令安装饥荒联机版的在当前目录下：\nforce_install_dir ./dontstarvetogether_dedicated_server 五、创建服务器存档 1、创建地图 在Windows上先创建地图，然后找到地图文件上传到服务器中。\n在Windows上打开饥荒联机版，在选单界面点击创建游戏对联机需要创建的世界进行配置，服务器模式选择公共，配置好密码，选择服务器设置，并勾选要使用的MOD。\n配置完成后，点击“生成世界”，等待世界创建完成，在进入选择人物界面后，就可以退出了。\n2、拷贝地图 在服务器中创建存档的存放文件夹。\nmkdir -p ~/.klei/DoNotStarveTogether/Cluster_1 在Windows中的C:\\Users\\{user_name}\\Documents\\Klei\\DoNotStarveTogether文件夹下，找到你刚才创建的存档，将所有内容上传到云服务器中的/.klei/DoNotStarveTogether/Cluster_1文件夹里。\n3、创建服务器token 进入科雷用户管理页面，点击导航栏中的游戏，看到饥荒后点击《饥荒：联机版》的游戏服务器，输入名称点击新增服务器，得到服务器token。\n在服务器的Cluster_1目录下创建文件cluster_token.txt, 将服务器token保存到文件中。\nvim cluster_token.txt // 按i进入输入模式 // 将生成的 pds-g 开头的 token 复制进该文件 // 输入:wq保存 4、添加管理员权限 进入科雷用户管理页面，点击导航栏中的用户信息，查看自己的Klei用户ID\n同样的创建管理员权限文件，复制kleiID到文件中\nvim adminlist.txt // 按i进入输入模式 // 将 Klei ID 复制进该文件 // 输入:wq保存 5、给服务器订阅mod 使用以下在线服务dst-mod-config-converter将已经创建的服务器中的modoverrides.lua文件转换为dedicated_server_mods_setup.lua文件中所需的内容，使用方法如下：\n从客户端找到 modoverrides.lua 文件 通常位于：.klei/DoNotStarveTogether/客户端设置/Cluster_1/Master/modoverrides.lua 和 .klei/DoNotStarveTogether/客户端设置/Cluster_1/Caves/modoverrides.lua 将文件内容粘贴到dst-mod-config-converter对应的输入框，即可在线快速转换 点击生成按钮 复制生成的内容到服务器的 dedicated_server_mods_setup.lua cd /dontstarvetogether_dedicated_server/mods vim dedicated_server_mods_setup.lua // 按i进入输入模式 // 将生成的内容复制进该文件 // 输入:wq保存 6、 编写启动脚本 回到根目录，创建boot.sh文件\ncd ~ vim boot.sh 把下面的命令粘贴到boot.sh中\n#!/bin/bash # steamcmd路径 steamcmd_dir=\u0026#34;$HOME/Steam\u0026#34; # 饥荒联机版安装路径 install_dir=\u0026#34;$HOME/dontstarvetogether_dedicated_server\u0026#34; cluster_name=\u0026#34;Cluster_1\u0026#34; dontstarve_dir=\u0026#34;$HOME/.klei/DoNotStarveTogether\u0026#34; function fail() { echo Error: \u0026#34;$@\u0026#34; \u0026gt;\u0026amp;2 exit 1 } function check_for_file() { if [ ! -e \u0026#34;$1\u0026#34; ]; then fail \u0026#34;Missing file: $1\u0026#34; fi } cd \u0026#34;$steamcmd_dir\u0026#34; || fail \u0026#34;Missing $steamcmd_dir directory!\u0026#34; check_for_file \u0026#34;steamcmd.sh\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/cluster.ini\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/cluster_token.txt\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/Master/server.ini\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/Caves/server.ini\u0026#34; check_for_file \u0026#34;$install_dir/bin\u0026#34; cd \u0026#34;$install_dir/bin\u0026#34; || fail run_shared=(./dontstarve_dedicated_server_nullrenderer) run_shared+=(-console) run_shared+=(-cluster \u0026#34;$cluster_name\u0026#34;) run_shared+=(-monitor_parent_process $$) run_shared+=(-shard) \u0026#34;${run_shared[@]}\u0026#34; Caves | sed \u0026#39;s/^/Caves: /\u0026#39; \u0026amp; \u0026#34;${run_shared[@]}\u0026#34; Master | sed \u0026#39;s/^/Master: /\u0026#39; 脚本中默认启动的是32位服务器，如果想启动64位服务，只需要更改这两行。\n# 32bit cd \u0026#34;$install_dir/bin\u0026#34; || fail run_shared=(./dontstarve_dedicated_server_nullrenderer) # 64bit cd \u0026#34;$install_dir/bin64\u0026#34; || fail run_shared=(./dontstarve_dedicated_server_nullrenderer_x64) 赋予boot.sh执行权限\nsudo chmod u+x boot.sh 到这里，饥荒服务器的部署就全部完成了，接下来还要开放端口以供其他人访问。\n六、开放端口 具体需要开放的端口可以查看存档文件夹下的这几个文件\n# 饥荒端口，UDP协议 ~/.klei/DoNotStarveTogether/Cluster_1/cluster.ini 中的 master_port。 ~/.klei/DoNotStarveTogether/Cluster_1/Master/server.ini 中的 server_port。 ~/.klei/DoNotStarveTogether/Cluster_1/Caves/server.ini 中的 server_port # steam端口，TCP+UDP协议 ~/.klei/DoNotStarveTogether/Cluster_1/Caves/server.ini 中的 master_server_port、authentication 注意开放的协议，饥荒端口都是UDP协议，steam的保险起见两种协议都开。\n七、启动饥荒服务器 到根目录，也就是boot.sh所在的目录，运行下面的代码\nnohup ./boot.sh\u0026gt;root.log 2\u0026gt;\u0026amp;1 \u0026amp; 执行后饥荒服务器会在后台运行，可以通过下面这个命令查看输出的日志\ntail -f root.log 八、关闭服务器 因为是后台运行，关闭服务器需要先找到服务器的PID，然后用kill命令杀死\nps -ef | grep don kill [pid] 相关链接 https://developer.valvesoftware.com/wiki/Zh/SteamCMD\nhttps://zhuanlan.zhihu.com/p/625645476\nhttps://accounts.klei.com/account/info\nhttps://github.com/ACAne0320/dst-mod-config-converter\nhttps://dst-mod-config-converter.vercel.app\n","permalink":"https://blog.nyaashino.com/post/dst_server_configuration/","summary":"饥荒服务器配置 服务器配置 腾讯云轻量级服务器\n2C4G\n系统盘 - SSD云硬盘 70GB\n600GB/月（带宽：6Mbps）\n镜像：Ubuntu22.04-Docker26\n一、下载SteamCMD 1、创建新用户 创建一个名为 steam 的用户帐户以安全地运行 SteamCMD，并将其与操作系统的其余部分隔离。以 root 用户身份登录时请勿运行 steamcmd——这样做会带来安全风险。\n1.以 root 用户身份创建 steam 用户： useradd -m steam or sudo useradd -m steam 2.设置密码 sudo passwd steam 3.切换到steam用户 su - steam 看到 $ 提示符就算切换成功\nubuntu@VM-8-7-ubuntu:~$ su - steam Password: $ $ 你可以通过运行以下命令确认当前用户：\nwhoami 输出应为：\nsteam 你也可以通过以下命令确认当前目录：\npwd 输出应为：\n/home/steam 4.使用管理员账户为 steam 用户赋予 sudo 权限 1.切换回管理员账户（如 ubuntu 用户）：\nexit 2.确认当前用户有 sudo 权限： 输入以下命令检查：","title":"饥荒服务器配置"},{"content":"前言 不知不觉2024年也过去了一大半了，公司的各种变动让我丧失了对工作的热情，学不到新的技术，没有新的需求，要做的仅仅是运营简单的机器学习项目，我时常思考这样的工作能为我带来什么？我的领导对我很好，我也很感谢他对我的培养，但是……\n鼓起勇气 在听闻需要调整薪资时，我脑袋里一片空白，之前想着公司在我最困难的时候愿意让我进去学习，而我能为公司做些什么呢？能陪伴他多久呢？现在看来，对于我本就不高的工资，在压力巨大的今天，抛去房租等日常开销，都不能随心所欲的想吃什么就吃什么了。 承诺的一年调薪两次也并没有做到，甚至还要降薪。好像是我不能接受的，我思来想去。\n在收到薪资调整意向的一个月之后，我才下定决心找下一份工作。\n看不到技术进步的工作、一趟一个半小时的通勤、需要调整的薪资，是压死骆驼的最后一根稻草。\n努力……会有回报吗？ 我不知道。\n下班的路上，同事和我说：\n“我们是在这里干了这么久了，也不太好找下家。你不一样，趁着你还年轻，去拼一下吧。”\n是啊，还年轻，是时候拼一下了。\n第一次面试网易外包给我的打击太大了，我发现我好像什么都不会。于是这一个月以来，七点起床，一两点才睡觉，每天都很疲惫，下班后修改简历、背八股文、看书了解Python高级用法(Fluent Python)、了解k8s/Docker等等……\n几乎没有其他额外开销的我，居然也攒不下什么钱，勉强够下一个月的房租。 我甚至怀疑是不是我的水平只够一份月薪7500的工作？噢不……甚至还要降。\n在面试了几家之后，我觉得我的水平对应的薪资远不止如此，我觉得我可以有更大的天地……如果我面试不紧张能把脑子里的知识都好好表达出来的话。\n勇敢迈向未来 我开始疯狂投简历找面试的机会，我想给我自己/给我爱的人更好的生活，累点苦点也无所谓。\n我从未想过在工作了这么久之后，还是在天天吃泡面，不敢多花钱，更别说攒下一笔钱了。\n既然选择了，就努力的去实现它，无论结果如何。\n过程永远都会让你切实地感觉到你自己的进步。\nTruth the Process.\n结语 希望在你迷茫的时候，找到一个目标，无论目标有多难，首先得先去做。在执行的过程中，一步一步接近自己的目标，回头来看，你会感谢那个勇于尝试的自己。\n","permalink":"https://blog.nyaashino.com/post/2024_review/","summary":"前言 不知不觉2024年也过去了一大半了，公司的各种变动让我丧失了对工作的热情，学不到新的技术，没有新的需求，要做的仅仅是运营简单的机器学习项目，我时常思考这样的工作能为我带来什么？我的领导对我很好，我也很感谢他对我的培养，但是……\n鼓起勇气 在听闻需要调整薪资时，我脑袋里一片空白，之前想着公司在我最困难的时候愿意让我进去学习，而我能为公司做些什么呢？能陪伴他多久呢？现在看来，对于我本就不高的工资，在压力巨大的今天，抛去房租等日常开销，都不能随心所欲的想吃什么就吃什么了。 承诺的一年调薪两次也并没有做到，甚至还要降薪。好像是我不能接受的，我思来想去。\n在收到薪资调整意向的一个月之后，我才下定决心找下一份工作。\n看不到技术进步的工作、一趟一个半小时的通勤、需要调整的薪资，是压死骆驼的最后一根稻草。\n努力……会有回报吗？ 我不知道。\n下班的路上，同事和我说：\n“我们是在这里干了这么久了，也不太好找下家。你不一样，趁着你还年轻，去拼一下吧。”\n是啊，还年轻，是时候拼一下了。\n第一次面试网易外包给我的打击太大了，我发现我好像什么都不会。于是这一个月以来，七点起床，一两点才睡觉，每天都很疲惫，下班后修改简历、背八股文、看书了解Python高级用法(Fluent Python)、了解k8s/Docker等等……\n几乎没有其他额外开销的我，居然也攒不下什么钱，勉强够下一个月的房租。 我甚至怀疑是不是我的水平只够一份月薪7500的工作？噢不……甚至还要降。\n在面试了几家之后，我觉得我的水平对应的薪资远不止如此，我觉得我可以有更大的天地……如果我面试不紧张能把脑子里的知识都好好表达出来的话。\n勇敢迈向未来 我开始疯狂投简历找面试的机会，我想给我自己/给我爱的人更好的生活，累点苦点也无所谓。\n我从未想过在工作了这么久之后，还是在天天吃泡面，不敢多花钱，更别说攒下一笔钱了。\n既然选择了，就努力的去实现它，无论结果如何。\n过程永远都会让你切实地感觉到你自己的进步。\nTruth the Process.\n结语 希望在你迷茫的时候，找到一个目标，无论目标有多难，首先得先去做。在执行的过程中，一步一步接近自己的目标，回头来看，你会感谢那个勇于尝试的自己。","title":"Do it for youself"},{"content":"项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file\n项目hugging face地址：\n一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。\nconda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。\nimport torch torch.cuda.is_available() 如果返回True，则cuda安装正常。\n如果返回False，请检查cuda和pytorch版本是否符合。\n使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。\n进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。\npip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。\n2、启动图形界面 再在LLaMA-Factory项目目录下使用以下命令启动图形用户界面：\npython src/train_web.py 三、选择模型进行微调 1、模型选择 在LLaMA-Factory中对以下开源LLM模型进行了适配：\n模型名 模型大小 默认模块 Template Baichuan2 7B/13B W_pack baichuan2 BLOOM 560M/1.1B/1.7B/3B/7.1B/176B query_key_value - BLOOMZ 560M/1.1B/1.7B/3B/7.1B/176B query_key_value - ChatGLM3 6B query_key_value chatglm3 DeepSeek (MoE) 7B/16B/67B q_proj,v_proj deepseek Falcon 7B/40B/180B query_key_value falcon Gemma 2B/7B q_proj,v_proj gemma InternLM2 7B/20B wqkv intern2 LLaMA 7B/13B/33B/65B q_proj,v_proj - LLaMA-2 7B/13B/70B q_proj,v_proj llama2 Mistral 7B q_proj,v_proj mistral Mixtral 8x7B q_proj,v_proj mistral OLMo 1B/7B att_proj olmo Phi-1.5/2 1.3B/2.7B q_proj,v_proj - Qwen 1.8B/7B/14B/72B c_attn qwen Qwen1.5 0.5B/1.8B/4B/7B/14B/72B q_proj,v_proj qwen StarCoder2 3B/7B/15B q_proj,v_proj - XVERSE 7B/13B/65B q_proj,v_proj xverse Yi 6B/9B/34B q_proj,v_proj yi Yuan 2B/51B/102B q_proj,v_proj yuan 当然，你也可以使用本地的模型进行微调。只需在模型路径中指定本地模型的文件路径即可。\n2、微调方法选择 full：对整个预训练模型进行微调，包括所有模型参数。 freeze：只使用少部分参数进行训练，把模型的大部分参数冻结。 lora：插入少量参数，只在新插入的参数上进行微调，达到加速效果。冻结预训练模型权重，将可训练的秩分解矩阵注入到Transformer层每个权重中。 3、训练方法选择 方法 全参数训练 部分参数训练 LoRA QLoRA 预训练 ✅ ✅ ✅ ✅ 指令监督微调 ✅ ✅ ✅ ✅ 奖励模型训练 ✅ ✅ ✅ ✅ PPO 训练 ✅ ✅ ✅ ✅ DPO 训练 ✅ ✅ ✅ ✅ ORPO 训练 ✅ ✅ ✅ ✅ 请使用 --quantization_bit 4 参数来启用 QLoRA 训练。\n4、数据集选择 使用方法请参考 data/README_zh.md 文件。\n部分数据集的使用需要确认，我们推荐使用下述命令登录您的 Hugging Face 账户。\npip install --upgrade huggingface_hub huggingface-cli login 自定义数据集 关于数据集文件的格式，请参考 data/README_zh.md 的内容。构建自定义数据集时，既可以使用单个 .json 文件，也可以使用一个数据加载脚本和多个文件。\n使用自定义数据集时，请更新 data/dataset_info.json 文件，该文件的格式请参考 data/README_zh.md。\n四、模型训练 当选择好训练参数后，点击开始即可进行模型训练。\n在图形界面中，可以看到训练进度、日志以及实时显示的损失值变化。\n五、模型推理 当训练完成后，点击刷新适配器，在适配器路径选择刚刚训练好的模型。\n在Chat中点击加载模型。\n输入想要提问的问题，进行测试。\n","permalink":"https://blog.nyaashino.com/post/llamafactory_quickstart/","summary":"项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file\n项目hugging face地址：\n一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。\nconda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。\nimport torch torch.cuda.is_available() 如果返回True，则cuda安装正常。\n如果返回False，请检查cuda和pytorch版本是否符合。\n使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。\n进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。\npip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。","title":"LLaMA-Factory Quickstart"},{"content":"既然都建好自己的博客了，总得记录些什么吧o.0，于是乎……\n今天是一个值得纪念的日子，为什么捏？请看~\n一、工作 从入职到今天正好四个月，也顺利度过了试用期，工资也勉强够生活（虽然除去必要的开销之后所剩无几了，而且还需要还前几个月一点收入的没有的时候的债，在此特别感谢愿意借我钱的好哥们，虽然你们不一定能读到这篇文章，虽然我还钱还的很慢，但是我会努力的QAQ）。工作氛围也好，压力也没那么大，很幸运遇到这样的公司。\n二、生活 今天搬进新房子啦，是一个一室一厅，虽然小了点，但我还是很喜欢的，租过程很愉快，中介也好，房东也好，都是很有爱的人。可能我的好运都用在这些地方了吧，从得到这份工作开始，最近也很少抱怨生活了，慢慢开始变得积极向上了。很感谢当时对我提供帮助的各位老师们，希望我的表现并没有让你们失望（虽然也不一定能读到这篇文章，我要点名表扬农老师！虽然我当面一直说不出那句感谢😄）。\n三、新的开始 成功转正了，也租了属于自己的一间房子，真正出来工作之后才知道原来生活这么困难，光是供自己活下去都费尽全力了。 所以现在我很敬佩那些靠自己的能力努力活到现在并且保持乐观的人，我也会努力的向他们看齐。\n插个今天搬家的小故事：\n今天搬家的时候叫了一个货拉拉的小面包车，司机也是一个努力、乐观又可爱的人，和我一样也有点内向，但是我能听出来他的很多话都是发自内心的，可能这就是同性相吸吧。出小区的时候，由于停车超过了半个小时，要付停车费，我说我转给你吧，他说不用了，大家都挺不容易的。那一瞬间我有点感动，但也只是笑了笑说这样不好吧，直到最后他也没收我的钱。我也只能回几声感谢，一想到可能再也没有缘分再见了，有点遗憾呢。但是人生就是这样在一段又一段遗憾的中度过的，谁又能一帆风顺呢？\nKeep calm and carry on!\n虽然生活很难，但我们还是要继续前行。\n努力变成乐观又元气满满的样子，谁能想到现在这样的我，在一年以前，是一个门都不敢出的玉玉症+自闭症+社恐+回避型依恋的人呢？很感谢过年的时候一直思考着是否要出去打拼、奋斗的自己，未来的你向你报喜啦，你成功了！虽然现在也很自闭，但是相比以前，已经好太多了，我能看见我自己的进步，所以我并不觉得我比别人差。\n最近加上午休每天也只睡了大概七个小时，睡眠质量极差。\n希望搬到新的环境中会有所改善吧🙏\n4、END 最后送大家正好听到的一首歌的一段歌词：\n只能由我自己来踏出这一步，不能在水边傻傻等待明天到来。\n","permalink":"https://blog.nyaashino.com/post/new_begining/","summary":"既然都建好自己的博客了，总得记录些什么吧o.0，于是乎……\n今天是一个值得纪念的日子，为什么捏？请看~\n一、工作 从入职到今天正好四个月，也顺利度过了试用期，工资也勉强够生活（虽然除去必要的开销之后所剩无几了，而且还需要还前几个月一点收入的没有的时候的债，在此特别感谢愿意借我钱的好哥们，虽然你们不一定能读到这篇文章，虽然我还钱还的很慢，但是我会努力的QAQ）。工作氛围也好，压力也没那么大，很幸运遇到这样的公司。\n二、生活 今天搬进新房子啦，是一个一室一厅，虽然小了点，但我还是很喜欢的，租过程很愉快，中介也好，房东也好，都是很有爱的人。可能我的好运都用在这些地方了吧，从得到这份工作开始，最近也很少抱怨生活了，慢慢开始变得积极向上了。很感谢当时对我提供帮助的各位老师们，希望我的表现并没有让你们失望（虽然也不一定能读到这篇文章，我要点名表扬农老师！虽然我当面一直说不出那句感谢😄）。\n三、新的开始 成功转正了，也租了属于自己的一间房子，真正出来工作之后才知道原来生活这么困难，光是供自己活下去都费尽全力了。 所以现在我很敬佩那些靠自己的能力努力活到现在并且保持乐观的人，我也会努力的向他们看齐。\n插个今天搬家的小故事：\n今天搬家的时候叫了一个货拉拉的小面包车，司机也是一个努力、乐观又可爱的人，和我一样也有点内向，但是我能听出来他的很多话都是发自内心的，可能这就是同性相吸吧。出小区的时候，由于停车超过了半个小时，要付停车费，我说我转给你吧，他说不用了，大家都挺不容易的。那一瞬间我有点感动，但也只是笑了笑说这样不好吧，直到最后他也没收我的钱。我也只能回几声感谢，一想到可能再也没有缘分再见了，有点遗憾呢。但是人生就是这样在一段又一段遗憾的中度过的，谁又能一帆风顺呢？\nKeep calm and carry on!\n虽然生活很难，但我们还是要继续前行。\n努力变成乐观又元气满满的样子，谁能想到现在这样的我，在一年以前，是一个门都不敢出的玉玉症+自闭症+社恐+回避型依恋的人呢？很感谢过年的时候一直思考着是否要出去打拼、奋斗的自己，未来的你向你报喜啦，你成功了！虽然现在也很自闭，但是相比以前，已经好太多了，我能看见我自己的进步，所以我并不觉得我比别人差。\n最近加上午休每天也只睡了大概七个小时，睡眠质量极差。\n希望搬到新的环境中会有所改善吧🙏\n4、END 最后送大家正好听到的一首歌的一段歌词：\n只能由我自己来踏出这一步，不能在水边傻傻等待明天到来。","title":"New Beginning!"},{"content":"BentoML 基本流程：\ntrain.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。\nbentoml.(framename).save_model(modelname, model)\nframename取决于你构建机器学习模型时使用的框架。\nmodelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。\nmodel为你所构建的模型的变量名\n假如我此前已经定义了两个变量：\nmnist_clf = 'tensorflow_mnist'\nmnist_model = tf.keras.models.load_model('models/mnist_model.h5')\n那么有以下保存模型的例子：\nbentoml.tensorflow.save_model(mnist_clf, mnist_model)\ne.g. 基于sklearn实现的iris数据集模型：\nimport bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=\u0026#39;scale\u0026#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.sklearn.save_model(\u0026#34;iris_clf\u0026#34;, clf) print(f\u0026#34;Model saved: {saved_model}\u0026#34;) # Model saved: Model(tag=\u0026#34;iris_clf:zy3dfgxzqkjrlgxi\u0026#34;) 以下是BentoML目前支持的机器学习框架：\nCatBoost Diffusers fast.ai Keras LightGBM MLflow ONNX PyTorch PyTorch Lightning Scikit-Learn TensorFlow Transformers XGBoost Detectron2 EasyORC 二、创建服务 创建一个service.py文件以提供服务：\n# service.py import numpy as np from PIL.Image import Image as PILImage import bentoml from bentoml.io import Image from bentoml.io import NumpyNdarray # 实例化runner对象 mnist_runner = bentoml.tensorflow.get(\u0026#34;tensorflow_mnist:latest\u0026#34;).to_runner() # 创建服务 svc = bentoml.Service( name=\u0026#34;tensorflow_mnist_demo\u0026#34;, runners=[mnist_runner], ) # 提供预测服务的函数 @svc.api(input=Image(), output=NumpyNdarray(dtype=\u0026#34;float32\u0026#34;)) async def predict_image(f: PILImage) -\u0026gt; \u0026#34;np.ndarray\u0026#34;: assert isinstance(f, PILImage) arr = np.array(f) / 255.0 assert arr.shape == (28, 28) # We are using greyscale image and our PyTorch model expect one # extra channel dimension arr = np.expand_dims(arr, (0, 3)).astype(\u0026#34;float32\u0026#34;) # reshape to [1, 28, 28, 1] return await mnist_runner.async_run(arr) 现在，我们有了一个可以对MNIST手写数字识别数据集中的图片进行预测的BentoML服务了。\n三、构建Bento 一旦定义了服务，我们就可以将模型和服务制作成bento，Bento是服务的发布格式，它是一个独立的归档文件，运行服务所需的所有源代码、模型文件和依赖关系规范。\n要构建Bento，首先要在项目目录中创建一个bentofile.yaml文件：\nservice: \u0026#34;service:svc\u0026#34; description: \u0026#34;file: ./README.md\u0026#34; labels: owner: bentoml-team stage: demo include: - \u0026#34;*.py\u0026#34; exclude: - \u0026#34;locustfile.py\u0026#34; python: lock_packages: false packages: - tensorflow - Pillow BentoML在bentofile.yaml中提供了大量构建选项，用于自定义Python依赖关系、cuda安装、docker镜像分发等等。更多有关bentofile.yaml选项的信息请点击构建Bentos。\n接下来就可以在包含service.py和bentofile.yaml文件的目录下构建bento了！\n运行bentoml build CLI 命令：\n$ bentoml build ██████╗ ███████╗███╗ ██╗████████╗ ██████╗ ███╗ ███╗██╗ ██╔══██╗██╔════╝████╗ ██║╚══██╔══╝██╔═══██╗████╗ ████║██║ ██████╔╝█████╗ ██╔██╗ ██║ ██║ ██║ ██║██╔████╔██║██║ ██╔══██╗██╔══╝ ██║╚██╗██║ ██║ ██║ ██║██║╚██╔╝██║██║ ██████╔╝███████╗██║ ╚████║ ██║ ╚██████╔╝██║ ╚═╝ ██║███████╗ ╚═════╝ ╚══════╝╚═╝ ╚═══╝ ╚═╝ ╚═════╝ ╚═╝ ╚═╝╚══════╝ Successfully built Bento(tag=\u0026#34;tensorflow_mnist_demo:n5g45ibme2efgedi\u0026#34;). Possible next steps: * Containerize your Bento with `bentoml containerize`: $ bentoml containerize tensorflow_mnist_demo:n5g45ibme2efgedi [or bentoml build --containerize] * Push to BentoCloud with `bentoml push`: $ bentoml push tensorflow_mnist_demo:n5g45ibme2efgedi [or bentoml build --push] 当然 你也可以指定需要构建的bento：\nbentoml build -f ./src/my_project_a/bento_fraud_detect.yaml ./src/\n和保存模型类似，新创建的bento也会自动生成唯一的版本标签。\n你可以使用bentoml list查看你在本地构建的所有bento\n当你有多个bento的时候，你是否需要清理一些不需要的bento？\n你可以使用bentoml delete {bentoname:version}来删除不需要的bento\n四、进行预测 1、提供服务 目前已知的有两种提供服务的方式：\n本地使用命令行提供服务 首先使用bentoml serve {bentoname:version} CLI命令来运行它，得到一个bentoserver：\n此时我们向localhost:3000/{svc.api_name}发送请求时，携带我们需要让其进行预测的数据以及数据类型，便可以获得相应的预测结果了。\n将模型推送到Yatai上并部署 在bento build完成之后，可以将模型push到yatai上，在Yatai上根据提示部署模型。设置相应的服务器端口便可以使用预测服务了。\n2、进行预测 目前已知的有三种进行预测的方式：\n不使用服务，直接加载本地使用bentoml.{framename}.save_model方式构建出来的模型，再进行预测（目前eb使用的就是这种方式 import bentoml tensorflow_mnist_runner = bentoml.tensorflow.get(\u0026#34;tensorflow_mnist:latest\u0026#34;).to_runner() tensorflow_mnist_runner.init_local() tensorflow_mnist_runner.run(image) 使用服务，发送请求，通过服务进行预测从而获取结果 import requests requests_url = \u0026#34;http://127.0.0.1:3000/predict_image\u0026#34; def predict(img_url, request_url): # 获取二进制的url with open(img_url, \u0026#39;rb\u0026#39;) as f: img_bytes = f.read() # 向创建的服务发送预测请求 包含 请求地址、请求头（内容类型）、以及预测的数据 result = requests.post( \u0026#34;http://127.0.0.1:3000/predict_image\u0026#34;, headers={\u0026#34;content-type\u0026#34;: \u0026#34;image/png\u0026#34;}, data=img_bytes, ).text # 将预测结果转化为对应标签 result = eval(result)[0] max_value = max(result) max_index = result.index(max_value) # 返回预测出来的标签 return max_index results = [] for i in range(10): result = predict(f\u0026#39;./samples/{i}.png\u0026#39;, requests_url) results.append(result) print(results) \u0026gt;\u0026gt; output:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 使用Swagger UI，交互式的发送预测请求 点击Try it out:\n选择图片发送请求：\n查看返回结果：\n五、Swagger UI 基本流程如下：\n在构建Bento的时候在bento_name/version/apis/文件夹下生成了openapi.yaml文件，生成的openapi.yaml文件它遵循以下规范。 当使用bentoml serve在本地启动了BentoML服务或者在Yatai上部署后，它便会将openapi.yaml文档自动提供给Swagger UI，使其可以在前端渲染。 其中的docs.json是由Swagger UI从BentoML服务的OpenAPI规范文件自动生成的。 Swagger UI 中自动生成的内容\nsrc/bentoml/_internal/service/openapi/__init__.py\nopenapi.yaml文件生成的代码路径\nsrc/bentoml/_internal/bento/bento.py\n基于__init__.py中的函数openapi_spec构建\n定义了服务上传多个文件输入/输出的 API 规范\nsrc/bentoml/_internal/io_descriptors/multipart.py\n定义端点和前端内容\nsrc/bentoml/_internal/server/http_app.py\n","permalink":"https://blog.nyaashino.com/post/bentoml_quickstart/","summary":"BentoML 基本流程：\ntrain.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。\nbentoml.(framename).save_model(modelname, model)\nframename取决于你构建机器学习模型时使用的框架。\nmodelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。\nmodel为你所构建的模型的变量名\n假如我此前已经定义了两个变量：\nmnist_clf = 'tensorflow_mnist'\nmnist_model = tf.keras.models.load_model('models/mnist_model.h5')\n那么有以下保存模型的例子：\nbentoml.tensorflow.save_model(mnist_clf, mnist_model)\ne.g. 基于sklearn实现的iris数据集模型：\nimport bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=\u0026#39;scale\u0026#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.","title":"BentoML使用流程"}]