[{"content":"饥荒服务器配置（2025/02/15更新） 基于腾讯云轻量的饥荒服务器配置\n解决启动服务器时下载Mod提示DownloadServerMods timed out with no response from Workshop...的错误。\n机器配置 主机配置 系统版本：Windows 11 23H2\n服务器 腾讯云轻量级服务器\n配置：2C4G\n存储：SSD云硬盘 70GB\n流量：600GB/月（带宽：6Mbps）\n镜像：Ubuntu22.04-Docker26\n推荐使用Termora进行SSH连接。\nTermora is a terminal emulator and SSH client for Windows, macOS and Linux.\n一、下载 steamcmd 1、安装相关依赖 安装steamcmd和饥荒服务器需要用到的依赖\nsudo add-apt-repository multiverse sudo dpkg --add-architecture i386 sudo apt update sudo apt install libstdc++6 libgcc1 libcurl4-gnutls-dev:i386 lib32z1 2、安装 steamcmd 在根目录下创建 Steam 文件夹\nmkdir ~/Steam \u0026amp;\u0026amp; cd ~/Steam 下载 steamcmd\nwget https://steamcdn-a.akamaihd.net/client/installer/steamcmd_linux.tar.gz 解压 steamcmd_linux.tar.gz 文件\ntar -xvzf steamcmd_linux.tar.gz 启动 steamcmd\n./steamcmd.sh 由于网络问题，更新可能很慢，推荐以下方式进行代理。\n如果你本地有开启相关代理服务，可以通过ssh隧道转发本地代理端口到服务器上，并在服务器设置代理相关环境变量。 ssh -CR 7890:localhost:7890 用户名@公网ip export https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7890 本地没有代理但是有相关订阅链接，推荐参考以下仓库对服务器进行代理。 https://github.com/mzz2017/gg 二、安装饥荒服务器 1、下载dst_dedicated_server 进入steamcmd后，参照以下命令输入：\nsteam\u0026gt; force_install_dir ../dontstarvetogether_dedicated_server steam\u0026gt; login anonymous steam\u0026gt; app_update 343050 validate steam\u0026gt; quit 2、登陆方式选择 方法1：匿名登录 下载大多数游戏服务端，你可以选择匿名登录。\nlogin anonymous 方法2：使用 Steam 帐户登录 一些服务端需要你使用一个 Steam 帐户来登录。\n注意：\n为了安全起见建议您创建一个服务端专用的 Steam 帐户。\n一个用户同时只能在一处登录（无论是在图形界面客户端还是在 SteamCMD 登录）。\nlogin \u0026lt;用户名\u0026gt; 下一步请输入您的密码。\n如果 Steam 令牌已经在用户帐户上激活，查看你电子邮箱中的 Steam 令牌验证码并输入它。这仅仅只是第一次登录时需要（以及当你删除了 SteamCMD 用于存储登录信息文件的时候）。\n如果你已经成功登录了你的帐户，你应该会看到一条消息。\n三、创建服务器存档 1、创建地图 在Windows上先创建地图，然后找到地图文件上传到服务器中。\n在Windows上打开饥荒联机版，在选单界面点击创建游戏对联机需要创建的世界进行配置，服务器模式选择公共，配置好密码，选择服务器设置，并勾选要使用的MOD。\n配置完成后，点击“生成世界”，等待世界创建完成，在进入选择人物界面后，就可以退出了。\n2、拷贝地图 在服务器中创建存档的存放文件夹。\nmkdir -p ~/.klei/DoNotStarveTogether/Cluster_1 在Windows中的C:\\Users\\{user_name}\\Documents\\Klei\\DoNotStarveTogether文件夹下，找到你刚才创建的存档，将所有内容上传到云服务器中的/.klei/DoNotStarveTogether/Cluster_1文件夹里。\n3、创建服务器token 进入科雷用户管理页面，点击导航栏中的游戏，看到饥荒后点击《饥荒：联机版》的游戏服务器，输入名称点击新增服务器，得到服务器token。\n在服务器的Cluster_1目录下创建文件cluster_token.txt, 将服务器token保存到文件中。\nvim cluster_token.txt // 按i进入输入模式 // 将生成的 pds-g 开头的 token 复制进该文件 // 输入:wq保存 4、添加管理员权限 进入科雷用户管理页面，点击导航栏中的用户信息，查看自己的Klei用户ID\n同样的创建管理员权限文件，复制kleiID到文件中\nvim adminlist.txt // 按i进入输入模式 // 将 Klei ID 复制进该文件 // 输入:wq保存 5、给服务器订阅mod 使用以下在线服务dst-mod-config-converter将已经创建的服务器中的modoverrides.lua文件转换为dedicated_server_mods_setup.lua文件中所需的内容，使用方法如下：\n从客户端找到 modoverrides.lua 文件 通常位于：.klei/DoNotStarveTogether/客户端设置/Cluster_1/Master/modoverrides.lua 和 .klei/DoNotStarveTogether/客户端设置/Cluster_1/Caves/modoverrides.lua 将文件内容粘贴到dst-mod-config-converter对应的输入框，即可在线快速转换 点击生成按钮 复制生成的内容到服务器的 dedicated_server_mods_setup.lua cd /dontstarvetogether_dedicated_server/mods vim dedicated_server_mods_setup.lua // 按i进入输入模式 // 将生成的内容复制进该文件 // 输入:wq保存 6、编写启动脚本 回到根目录，创建boot.sh文件\ncd ~ vim boot.sh 把下面的命令粘贴到boot.sh中\n#!/bin/bash # steamcmd路径 steamcmd_dir=\u0026#34;$HOME/Steam\u0026#34; # 饥荒联机版安装路径 install_dir=\u0026#34;$HOME/dontstarvetogether_dedicated_server\u0026#34; cluster_name=\u0026#34;Cluster_1\u0026#34; dontstarve_dir=\u0026#34;$HOME/.klei/DoNotStarveTogether\u0026#34; function fail() { echo Error: \u0026#34;$@\u0026#34; \u0026gt;\u0026amp;2 exit 1 } function check_for_file() { if [ ! -e \u0026#34;$1\u0026#34; ]; then fail \u0026#34;Missing file: $1\u0026#34; fi } cd \u0026#34;$steamcmd_dir\u0026#34; || fail \u0026#34;Missing $steamcmd_dir directory!\u0026#34; check_for_file \u0026#34;steamcmd.sh\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/cluster.ini\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/cluster_token.txt\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/Master/server.ini\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/Caves/server.ini\u0026#34; check_for_file \u0026#34;$install_dir/bin\u0026#34; cd \u0026#34;$install_dir/bin\u0026#34; || fail run_shared=(./dontstarve_dedicated_server_nullrenderer) run_shared+=(-console) run_shared+=(-cluster \u0026#34;$cluster_name\u0026#34;) run_shared+=(-monitor_parent_process $$) run_shared+=(-shard) \u0026#34;${run_shared[@]}\u0026#34; Caves | sed \u0026#39;s/^/Caves: /\u0026#39; \u0026amp; \u0026#34;${run_shared[@]}\u0026#34; Master | sed \u0026#39;s/^/Master: /\u0026#39; 脚本中默认启动的是32位服务器，如果想启动64位服务，只需要更改这两行。\n# 32bit cd \u0026#34;$install_dir/bin\u0026#34; || fail run_shared=(./dontstarve_dedicated_server_nullrenderer) # 64bit cd \u0026#34;$install_dir/bin64\u0026#34; || fail run_shared=(./dontstarve_dedicated_server_nullrenderer_x64) 赋予boot.sh执行权限\nsudo chmod u+x boot.sh 到这里，饥荒服务器的部署就全部完成了，接下来还要开放端口以供其他人访问。\n四、开放端口 具体需要开放的端口可以查看存档文件夹下的这几个文件\n# 饥荒端口，UDP协议 ~/.klei/DoNotStarveTogether/Cluster_1/cluster.ini 中的 master_port。 ~/.klei/DoNotStarveTogether/Cluster_1/Master/server.ini 中的 server_port。 ~/.klei/DoNotStarveTogether/Cluster_1/Caves/server.ini 中的 server_port # steam端口，TCP+UDP协议 ~/.klei/DoNotStarveTogether/Cluster_1/Caves/server.ini 中的 master_server_port、authentication 注意开放的协议，饥荒端口都是UDP协议，steam的保险起见两种协议都开。\n五、启动饥荒服务器 到根目录，也就是boot.sh所在的目录，运行下面的代码\nnohup ./boot.sh\u0026gt;root.log 2\u0026gt;\u0026amp;1 \u0026amp; 执行后饥荒服务器会在后台运行，可以通过下面这个命令查看输出的日志\ntail -f root.log 此时服务器在下载Mod时，日志可能出现DownloadServerMods timed out with no response from Workshop...的错误，请按照以下方式替换steamclient.so文件。\n将/dontstarvetogether_dedicated_server/linux64文件夹下的steamclient.so复制到/dontstarvetogether_dedicated_server/bin64/lib64文件夹。\ncp ~/dontstarvetogether_dedicated_server/linux64/steamclient.so ~/dontstarvetogether_dedicated_server//bin64/lib64/steamclient.so 再次重新启动服务器，mod可以正常下载了。\n六、关闭服务器 因为是后台运行，关闭服务器需要先找到服务器的PID，然后用kill命令杀死\nps -ef | grep don kill [pid] 七、使用 systemd 管理饥荒服务器启动脚本（可选） 基于此前的boot.sh 脚本，使用 systemd 管理饥荒服务器的完整配置和操作指南。\n1、准备工作 确保脚本路径：\n确认您的 boot.sh文件位于 /root/boot.sh，并且该脚本有执行权限。\nchmod +x /root/boot.sh 修改boot.sh文件\n在脚本顶部添加trap 'kill $(jobs -p)' SIGINT SIGTERM\n在脚本末尾添加 wait 命令，确保脚本运行时，主脚本不会在后台子进程还未退出时直接终止\n最终bash.sh文件如下：\n#!/bin/bash # steamcmd路径 steamcmd_dir=\u0026#34;$HOME/Steam\u0026#34; # 饥荒联机版安装路径 install_dir=\u0026#34;$HOME/dontstarvetogether_dedicated_server\u0026#34; cluster_name=\u0026#34;Cluster_1\u0026#34; dontstarve_dir=\u0026#34;$HOME/.klei/DoNotStarveTogether\u0026#34; # 在 boot.sh 中捕获终止信号，并确保所有子进程在停止时被正确清理 trap \u0026#39;kill $(jobs -p)\u0026#39; SIGINT SIGTERM function fail() { echo Error: \u0026#34;$@\u0026#34; \u0026gt;\u0026amp;2 exit 1 } function check_for_file() { if [ ! -e \u0026#34;$1\u0026#34; ]; then fail \u0026#34;Missing file: $1\u0026#34; fi } cd \u0026#34;$steamcmd_dir\u0026#34; || fail \u0026#34;Missing $steamcmd_dir directory!\u0026#34; check_for_file \u0026#34;steamcmd.sh\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/cluster.ini\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/cluster_token.txt\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/Master/server.ini\u0026#34; check_for_file \u0026#34;$dontstarve_dir/$cluster_name/Caves/server.ini\u0026#34; check_for_file \u0026#34;$install_dir/bin\u0026#34; cd \u0026#34;$install_dir/bin\u0026#34; || fail run_shared=(./dontstarve_dedicated_server_nullrenderer) run_shared+=(-console) run_shared+=(-cluster \u0026#34;$cluster_name\u0026#34;) run_shared+=(-monitor_parent_process $$) run_shared+=(-shard) \u0026#34;${run_shared[@]}\u0026#34; Caves | sed \u0026#39;s/^/Caves: /\u0026#39; \u0026amp; \u0026#34;${run_shared[@]}\u0026#34; Master | sed \u0026#39;s/^/Master: /\u0026#39; wait 创建日志目录：\n如果您的 boot.sh没有指定日志文件路径，建议设置一个日志目录，例如：\nmkdir -p /root/logs 2、创建 systemd 服务文件 创建服务文件：\n在 /etc/systemd/system/ 目录下创建一个服务文件 dst_server.service：\nsudo nano /etc/systemd/system/dst_server.service 填写服务配置：\n将以下内容复制到文件中，修改必要的路径：\n[Unit] Description=Don\u0026#39;t Starve Together Dedicated Server After=network.target [Service] User=root Group=root WorkingDirectory=/root ExecStart=/root/boot.sh ExecStop=/bin/kill -TERM $MAINPID KillMode=control-group Restart=on-failure RestartSec=5s StandardOutput=append:/root/logs/dst_server.log StandardError=append:/root/logs/dst_server_error.log [Install] WantedBy=multi-user.target 说明：\nUser 和 Group：指定运行服务的用户和用户组（此处为 root）。 WorkingDirectory：脚本的工作目录。 ExecStart：运行 boot.sh 脚本。 Restart=on-failure：在脚本失败时自动重启服务。 StandardOutput 和 StandardError：将标准输出和错误日志分别写入日志文件。 保存文件并退出： 按 Ctrl+O 保存，按 Ctrl+X 退出编辑器。\n重新加载 systemd 配置：\nsudo systemctl daemon-reload 3、启动和管理服务 启动服务：\nsudo systemctl start dst_server 设置开机自启：\nsudo systemctl enable dst_server 查看服务状态：\nsudo systemctl status dst_server 停止服务：\nsudo systemctl stop dst_server 重启服务：\nsudo systemctl restart dst_server 4、查看和分析日志 实时查看日志：\nsudo journalctl -u dst_server -f 查看特定时间段日志：\nsudo journalctl -u dst_server --since \u0026#34;2025-01-01 00:00:00\u0026#34; 查看日志文件： 如果使用 StandardOutput 和 StandardError 将日志记录到文件，可以通过 tail 查看：\ntail -f /root/logs/dst_server.log 5、注意事项 确保 steam 用户对 /root/boot.sh 和日志目录具有读写权限。\n如果修改了 dst_server.service文件内容，记得重新加载 systemd配置：\nsudo systemctl daemon-reload 如果服务无法正常启动，检查日志文件或运行以下命令查看错误：\nsudo journalctl -u dst_server 八、开启 swap 虚拟内存（可选） 如果Mod开启数量较多导致服务器内存溢出，可以考虑开启虚拟内存缓解内存压力。 推荐开启大小为你的服务器内存大小。\n1、Swap 配置方法 假设你要创建 4G Swap：\nsudo fallocate -l 4G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile echo \u0026#34;/swapfile none swap sw 0 0\u0026#34; | sudo tee -a /etc/fstab` 检查 Swap 是否生效：\nfree -h swapon --show 2、Swap 优化 如果 Swap 频繁被使用，可能需要优化 swappiness 值：\nsudo sysctl vm.swappiness=10 echo \u0026#34;vm.swappiness=10\u0026#34; | sudo tee -a /etc/sysctl.conf 这样能降低 Swap 频繁使用，提高系统性能。\n相关链接 https://developer.valvesoftware.com/wiki/Zh/SteamCMD\nhttps://zhuanlan.zhihu.com/p/625645476\nhttps://accounts.klei.com/account/info\nhttps://github.com/ACAne0320/dst-mod-config-converter\nhttps://github.com/TermoraDev/termora\nhttps://github.com/mzz2017/gg\nhttps://dst-mod-config-converter.vercel.app\nhttps://tieba.baidu.com/p/9251408506\n","permalink":"https://blog.nyaashino.com/post/dst_server_configuration/","summary":"饥荒服务器配置（2025/02/15更新） 基于腾讯云轻量的饥荒服务器配置\n解决启动服务器时下载Mod提示DownloadServerMods timed out with no response from Workshop...的错误。\n机器配置 主机配置 系统版本：Windows 11 23H2\n服务器 腾讯云轻量级服务器\n配置：2C4G\n存储：SSD云硬盘 70GB\n流量：600GB/月（带宽：6Mbps）\n镜像：Ubuntu22.04-Docker26\n推荐使用Termora进行SSH连接。\nTermora is a terminal emulator and SSH client for Windows, macOS and Linux.\n一、下载 steamcmd 1、安装相关依赖 安装steamcmd和饥荒服务器需要用到的依赖\nsudo add-apt-repository multiverse sudo dpkg --add-architecture i386 sudo apt update sudo apt install libstdc++6 libgcc1 libcurl4-gnutls-dev:i386 lib32z1 2、安装 steamcmd 在根目录下创建 Steam 文件夹\nmkdir ~/Steam \u0026amp;\u0026amp; cd ~/Steam 下载 steamcmd\nwget https://steamcdn-a.","title":"饥荒服务器配置（腾讯云）"},{"content":"本文档旨在给对于从来没有接触过GitHub Actions以及Cookies相关的网络知识的萌新提供简单易懂的网易云合伙人脚本基于GitHub Actions部署的操作方式。\n准备工作 Github账号，点此注册并登录 有音乐合伙人权益的网易云音乐账号，点此登录 实操步骤 1. Fork ncmp 仓库 项目地址：https://github.com/ACAne0320/ncmp 进入ncmp仓库点击Fork(前提已登录) 创建仓库\n仓库名称，可以随便填写 点击创建 2. 获取脚本所需相关参数 网易云Cookies获取 首先登录网易云音乐网页版，登录后按F12（或者鼠标右键）打开开发者模式 切换到网络页面 此时随便点击一下网页，发起请求（比如点击我的音乐） 在Network页面按照步骤操作\n打开搜索框 在搜索框中搜索_csrf或者MUSIC_U 找到需要的两个参数_csrf和MUSIC_U，复制下来，后面会用到 邮箱通知设置（以QQ邮箱为例） 进入QQ邮箱-\u0026gt;设置-\u0026gt;账号-\u0026gt;打开POP3/IMAP/SMTP/Exchange/CardDAV/CalDAV服务 记住该授权码，后续会用到\n此授权码仅用于自己给自己发送Cookie过期提示短信，请勿暴露\n3. 设置GitHub Actions Secret 在项目仓库的Settings-\u0026gt;Security-\u0026gt;Secrets and variables中设置Secret 然后将步骤2中获取的参数一一填写进去。\nName Secret MUSIC_U 网易云音乐 MUSIC_U CSRF 网易云音乐 csrf NOTIFY_EMAIL 你的邮箱地址（此例中为 xxxx@qq.com） EMAIL_PASSWORD 邮箱授权码（在上面步骤中获取的授权码） SMTP_SERVER smtp.qq.com（如果是qq，直接使用这个） SMTP_PORT 465（同上） 创建CSRF 创建MUSIC_U 其他参数同理填写。\n4. 手动运行脚本 点击进入Actions页面 点击I understand my workflows, go ahead and enable them 进入Auto Score，点击Enable workflow启用工作流 手动运行测试一次 可以点击进去查看详细日志 如果在前面的步骤中，设置了邮件相关参数的话，当Cookie过期时，会发送邮件提醒。此时再去Sercet中更新网易云音乐的两个Cookie 即可。\n至此，使用 Github Actions 运行网易云音乐合伙人脚本的全部步骤已完成。 如果在使用过程中出现了别的问题或者有什么疑问的话，欢迎小红书私聊/邮箱联系。\n如果该项目对你有帮助的话，点个免费的star吧，谢谢～ 相关链接 ncmp 项目地址\n网易云音乐\nQQ邮箱\n","permalink":"https://blog.nyaashino.com/post/ncmp_quickstart/","summary":"本文档旨在给对于从来没有接触过GitHub Actions以及Cookies相关的网络知识的萌新提供简单易懂的网易云合伙人脚本基于GitHub Actions部署的操作方式。\n准备工作 Github账号，点此注册并登录 有音乐合伙人权益的网易云音乐账号，点此登录 实操步骤 1. Fork ncmp 仓库 项目地址：https://github.com/ACAne0320/ncmp 进入ncmp仓库点击Fork(前提已登录) 创建仓库\n仓库名称，可以随便填写 点击创建 2. 获取脚本所需相关参数 网易云Cookies获取 首先登录网易云音乐网页版，登录后按F12（或者鼠标右键）打开开发者模式 切换到网络页面 此时随便点击一下网页，发起请求（比如点击我的音乐） 在Network页面按照步骤操作\n打开搜索框 在搜索框中搜索_csrf或者MUSIC_U 找到需要的两个参数_csrf和MUSIC_U，复制下来，后面会用到 邮箱通知设置（以QQ邮箱为例） 进入QQ邮箱-\u0026gt;设置-\u0026gt;账号-\u0026gt;打开POP3/IMAP/SMTP/Exchange/CardDAV/CalDAV服务 记住该授权码，后续会用到\n此授权码仅用于自己给自己发送Cookie过期提示短信，请勿暴露\n3. 设置GitHub Actions Secret 在项目仓库的Settings-\u0026gt;Security-\u0026gt;Secrets and variables中设置Secret 然后将步骤2中获取的参数一一填写进去。\nName Secret MUSIC_U 网易云音乐 MUSIC_U CSRF 网易云音乐 csrf NOTIFY_EMAIL 你的邮箱地址（此例中为 xxxx@qq.com） EMAIL_PASSWORD 邮箱授权码（在上面步骤中获取的授权码） SMTP_SERVER smtp.qq.com（如果是qq，直接使用这个） SMTP_PORT 465（同上） 创建CSRF 创建MUSIC_U 其他参数同理填写。\n4. 手动运行脚本 点击进入Actions页面 点击I understand my workflows, go ahead and enable them 进入Auto Score，点击Enable workflow启用工作流 手动运行测试一次 可以点击进去查看详细日志 如果在前面的步骤中，设置了邮件相关参数的话，当Cookie过期时，会发送邮件提醒。此时再去Sercet中更新网易云音乐的两个Cookie 即可。","title":"ncmp 使用指北"},{"content":"前言 不知不觉2024年也过去了一大半了，公司的各种变动让我丧失了对工作的热情，学不到新的技术，没有新的需求，要做的仅仅是运营简单的机器学习项目，我时常思考这样的工作能为我带来什么？我的领导对我很好，我也很感谢他对我的培养，但是……\n鼓起勇气 在听闻需要调整薪资时，我脑袋里一片空白，之前想着公司在我最困难的时候愿意让我进去学习，而我能为公司做些什么呢？能陪伴他多久呢？现在看来，对于我本就不高的工资，在压力巨大的今天，抛去房租等日常开销，都不能随心所欲的想吃什么就吃什么了。 承诺的一年调薪两次也并没有做到，甚至还要降薪。好像是我不能接受的，我思来想去。\n在收到薪资调整意向的一个月之后，我才下定决心找下一份工作。\n看不到技术进步的工作、一趟一个半小时的通勤、需要调整的薪资，是压死骆驼的最后一根稻草。\n努力……会有回报吗？ 我不知道。\n下班的路上，同事和我说：\n“我们是在这里干了这么久了，也不太好找下家。你不一样，趁着你还年轻，去拼一下吧。”\n是啊，还年轻，是时候拼一下了。\n第一次面试网易外包给我的打击太大了，我发现我好像什么都不会。于是这一个月以来，七点起床，一两点才睡觉，每天都很疲惫，下班后修改简历、背八股文、看书了解Python高级用法(Fluent Python)、了解k8s/Docker等等……\n几乎没有其他额外开销的我，居然也攒不下什么钱，勉强够下一个月的房租。 我甚至怀疑是不是我的水平只够一份月薪7500的工作？噢不……甚至还要降。\n在面试了几家之后，我觉得我的水平对应的薪资远不止如此，我觉得我可以有更大的天地……如果我面试不紧张能把脑子里的知识都好好表达出来的话。\n勇敢迈向未来 我开始疯狂投简历找面试的机会，我想给我自己/给我爱的人更好的生活，累点苦点也无所谓。\n我从未想过在工作了这么久之后，还是在天天吃泡面，不敢多花钱，更别说攒下一笔钱了。\n既然选择了，就努力的去实现它，无论结果如何。\n过程永远都会让你切实地感觉到你自己的进步。\nTruth the Process.\n结语 希望在你迷茫的时候，找到一个目标，无论目标有多难，首先得先去做。在执行的过程中，一步一步接近自己的目标，回头来看，你会感谢那个勇于尝试的自己。\n","permalink":"https://blog.nyaashino.com/post/2024_review/","summary":"前言 不知不觉2024年也过去了一大半了，公司的各种变动让我丧失了对工作的热情，学不到新的技术，没有新的需求，要做的仅仅是运营简单的机器学习项目，我时常思考这样的工作能为我带来什么？我的领导对我很好，我也很感谢他对我的培养，但是……\n鼓起勇气 在听闻需要调整薪资时，我脑袋里一片空白，之前想着公司在我最困难的时候愿意让我进去学习，而我能为公司做些什么呢？能陪伴他多久呢？现在看来，对于我本就不高的工资，在压力巨大的今天，抛去房租等日常开销，都不能随心所欲的想吃什么就吃什么了。 承诺的一年调薪两次也并没有做到，甚至还要降薪。好像是我不能接受的，我思来想去。\n在收到薪资调整意向的一个月之后，我才下定决心找下一份工作。\n看不到技术进步的工作、一趟一个半小时的通勤、需要调整的薪资，是压死骆驼的最后一根稻草。\n努力……会有回报吗？ 我不知道。\n下班的路上，同事和我说：\n“我们是在这里干了这么久了，也不太好找下家。你不一样，趁着你还年轻，去拼一下吧。”\n是啊，还年轻，是时候拼一下了。\n第一次面试网易外包给我的打击太大了，我发现我好像什么都不会。于是这一个月以来，七点起床，一两点才睡觉，每天都很疲惫，下班后修改简历、背八股文、看书了解Python高级用法(Fluent Python)、了解k8s/Docker等等……\n几乎没有其他额外开销的我，居然也攒不下什么钱，勉强够下一个月的房租。 我甚至怀疑是不是我的水平只够一份月薪7500的工作？噢不……甚至还要降。\n在面试了几家之后，我觉得我的水平对应的薪资远不止如此，我觉得我可以有更大的天地……如果我面试不紧张能把脑子里的知识都好好表达出来的话。\n勇敢迈向未来 我开始疯狂投简历找面试的机会，我想给我自己/给我爱的人更好的生活，累点苦点也无所谓。\n我从未想过在工作了这么久之后，还是在天天吃泡面，不敢多花钱，更别说攒下一笔钱了。\n既然选择了，就努力的去实现它，无论结果如何。\n过程永远都会让你切实地感觉到你自己的进步。\nTruth the Process.\n结语 希望在你迷茫的时候，找到一个目标，无论目标有多难，首先得先去做。在执行的过程中，一步一步接近自己的目标，回头来看，你会感谢那个勇于尝试的自己。","title":"Do it for youself"},{"content":"项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file\n项目hugging face地址：\n一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。\nconda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。\nimport torch torch.cuda.is_available() 如果返回True，则cuda安装正常。\n如果返回False，请检查cuda和pytorch版本是否符合。\n使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。\n进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。\npip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。\n2、启动图形界面 再在LLaMA-Factory项目目录下使用以下命令启动图形用户界面：\npython src/train_web.py 三、选择模型进行微调 1、模型选择 在LLaMA-Factory中对以下开源LLM模型进行了适配：\n模型名 模型大小 默认模块 Template Baichuan2 7B/13B W_pack baichuan2 BLOOM 560M/1.1B/1.7B/3B/7.1B/176B query_key_value - BLOOMZ 560M/1.1B/1.7B/3B/7.1B/176B query_key_value - ChatGLM3 6B query_key_value chatglm3 DeepSeek (MoE) 7B/16B/67B q_proj,v_proj deepseek Falcon 7B/40B/180B query_key_value falcon Gemma 2B/7B q_proj,v_proj gemma InternLM2 7B/20B wqkv intern2 LLaMA 7B/13B/33B/65B q_proj,v_proj - LLaMA-2 7B/13B/70B q_proj,v_proj llama2 Mistral 7B q_proj,v_proj mistral Mixtral 8x7B q_proj,v_proj mistral OLMo 1B/7B att_proj olmo Phi-1.5/2 1.3B/2.7B q_proj,v_proj - Qwen 1.8B/7B/14B/72B c_attn qwen Qwen1.5 0.5B/1.8B/4B/7B/14B/72B q_proj,v_proj qwen StarCoder2 3B/7B/15B q_proj,v_proj - XVERSE 7B/13B/65B q_proj,v_proj xverse Yi 6B/9B/34B q_proj,v_proj yi Yuan 2B/51B/102B q_proj,v_proj yuan 当然，你也可以使用本地的模型进行微调。只需在模型路径中指定本地模型的文件路径即可。\n2、微调方法选择 full：对整个预训练模型进行微调，包括所有模型参数。 freeze：只使用少部分参数进行训练，把模型的大部分参数冻结。 lora：插入少量参数，只在新插入的参数上进行微调，达到加速效果。冻结预训练模型权重，将可训练的秩分解矩阵注入到Transformer层每个权重中。 3、训练方法选择 方法 全参数训练 部分参数训练 LoRA QLoRA 预训练 ✅ ✅ ✅ ✅ 指令监督微调 ✅ ✅ ✅ ✅ 奖励模型训练 ✅ ✅ ✅ ✅ PPO 训练 ✅ ✅ ✅ ✅ DPO 训练 ✅ ✅ ✅ ✅ ORPO 训练 ✅ ✅ ✅ ✅ 请使用 --quantization_bit 4 参数来启用 QLoRA 训练。\n4、数据集选择 使用方法请参考 data/README_zh.md 文件。\n部分数据集的使用需要确认，我们推荐使用下述命令登录您的 Hugging Face 账户。\npip install --upgrade huggingface_hub huggingface-cli login 自定义数据集 关于数据集文件的格式，请参考 data/README_zh.md 的内容。构建自定义数据集时，既可以使用单个 .json 文件，也可以使用一个数据加载脚本和多个文件。\n使用自定义数据集时，请更新 data/dataset_info.json 文件，该文件的格式请参考 data/README_zh.md。\n四、模型训练 当选择好训练参数后，点击开始即可进行模型训练。\n在图形界面中，可以看到训练进度、日志以及实时显示的损失值变化。\n五、模型推理 当训练完成后，点击刷新适配器，在适配器路径选择刚刚训练好的模型。\n在Chat中点击加载模型。\n输入想要提问的问题，进行测试。\n","permalink":"https://blog.nyaashino.com/post/llamafactory_quickstart/","summary":"项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file\n项目hugging face地址：\n一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。\nconda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。\nimport torch torch.cuda.is_available() 如果返回True，则cuda安装正常。\n如果返回False，请检查cuda和pytorch版本是否符合。\n使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。\n进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。\npip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。","title":"LLaMA-Factory Quickstart"},{"content":"既然都建好自己的博客了，总得记录些什么吧o.0，于是乎……\n今天是一个值得纪念的日子，为什么捏？请看~\n一、工作 从入职到今天正好四个月，也顺利度过了试用期，工资也勉强够生活（虽然除去必要的开销之后所剩无几了，而且还需要还前几个月一点收入的没有的时候的债，在此特别感谢愿意借我钱的好哥们，虽然你们不一定能读到这篇文章，虽然我还钱还的很慢，但是我会努力的QAQ）。工作氛围也好，压力也没那么大，很幸运遇到这样的公司。\n二、生活 今天搬进新房子啦，是一个一室一厅，虽然小了点，但我还是很喜欢的，租过程很愉快，中介也好，房东也好，都是很有爱的人。可能我的好运都用在这些地方了吧，从得到这份工作开始，最近也很少抱怨生活了，慢慢开始变得积极向上了。很感谢当时对我提供帮助的各位老师们，希望我的表现并没有让你们失望（虽然也不一定能读到这篇文章，我要点名表扬农老师！虽然我当面一直说不出那句感谢😄）。\n三、新的开始 成功转正了，也租了属于自己的一间房子，真正出来工作之后才知道原来生活这么困难，光是供自己活下去都费尽全力了。 所以现在我很敬佩那些靠自己的能力努力活到现在并且保持乐观的人，我也会努力的向他们看齐。\n插个今天搬家的小故事：\n今天搬家的时候叫了一个货拉拉的小面包车，司机也是一个努力、乐观又可爱的人，和我一样也有点内向，但是我能听出来他的很多话都是发自内心的，可能这就是同性相吸吧。出小区的时候，由于停车超过了半个小时，要付停车费，我说我转给你吧，他说不用了，大家都挺不容易的。那一瞬间我有点感动，但也只是笑了笑说这样不好吧，直到最后他也没收我的钱。我也只能回几声感谢，一想到可能再也没有缘分再见了，有点遗憾呢。但是人生就是这样在一段又一段遗憾的中度过的，谁又能一帆风顺呢？\nKeep calm and carry on!\n虽然生活很难，但我们还是要继续前行。\n努力变成乐观又元气满满的样子，谁能想到现在这样的我，在一年以前，是一个门都不敢出的玉玉症+自闭症+社恐+回避型依恋的人呢？很感谢过年的时候一直思考着是否要出去打拼、奋斗的自己，未来的你向你报喜啦，你成功了！虽然现在也很自闭，但是相比以前，已经好太多了，我能看见我自己的进步，所以我并不觉得我比别人差。\n最近加上午休每天也只睡了大概七个小时，睡眠质量极差。\n希望搬到新的环境中会有所改善吧🙏\n4、END 最后送大家正好听到的一首歌的一段歌词：\n只能由我自己来踏出这一步，不能在水边傻傻等待明天到来。\n","permalink":"https://blog.nyaashino.com/post/new_begining/","summary":"既然都建好自己的博客了，总得记录些什么吧o.0，于是乎……\n今天是一个值得纪念的日子，为什么捏？请看~\n一、工作 从入职到今天正好四个月，也顺利度过了试用期，工资也勉强够生活（虽然除去必要的开销之后所剩无几了，而且还需要还前几个月一点收入的没有的时候的债，在此特别感谢愿意借我钱的好哥们，虽然你们不一定能读到这篇文章，虽然我还钱还的很慢，但是我会努力的QAQ）。工作氛围也好，压力也没那么大，很幸运遇到这样的公司。\n二、生活 今天搬进新房子啦，是一个一室一厅，虽然小了点，但我还是很喜欢的，租过程很愉快，中介也好，房东也好，都是很有爱的人。可能我的好运都用在这些地方了吧，从得到这份工作开始，最近也很少抱怨生活了，慢慢开始变得积极向上了。很感谢当时对我提供帮助的各位老师们，希望我的表现并没有让你们失望（虽然也不一定能读到这篇文章，我要点名表扬农老师！虽然我当面一直说不出那句感谢😄）。\n三、新的开始 成功转正了，也租了属于自己的一间房子，真正出来工作之后才知道原来生活这么困难，光是供自己活下去都费尽全力了。 所以现在我很敬佩那些靠自己的能力努力活到现在并且保持乐观的人，我也会努力的向他们看齐。\n插个今天搬家的小故事：\n今天搬家的时候叫了一个货拉拉的小面包车，司机也是一个努力、乐观又可爱的人，和我一样也有点内向，但是我能听出来他的很多话都是发自内心的，可能这就是同性相吸吧。出小区的时候，由于停车超过了半个小时，要付停车费，我说我转给你吧，他说不用了，大家都挺不容易的。那一瞬间我有点感动，但也只是笑了笑说这样不好吧，直到最后他也没收我的钱。我也只能回几声感谢，一想到可能再也没有缘分再见了，有点遗憾呢。但是人生就是这样在一段又一段遗憾的中度过的，谁又能一帆风顺呢？\nKeep calm and carry on!\n虽然生活很难，但我们还是要继续前行。\n努力变成乐观又元气满满的样子，谁能想到现在这样的我，在一年以前，是一个门都不敢出的玉玉症+自闭症+社恐+回避型依恋的人呢？很感谢过年的时候一直思考着是否要出去打拼、奋斗的自己，未来的你向你报喜啦，你成功了！虽然现在也很自闭，但是相比以前，已经好太多了，我能看见我自己的进步，所以我并不觉得我比别人差。\n最近加上午休每天也只睡了大概七个小时，睡眠质量极差。\n希望搬到新的环境中会有所改善吧🙏\n4、END 最后送大家正好听到的一首歌的一段歌词：\n只能由我自己来踏出这一步，不能在水边傻傻等待明天到来。","title":"New Beginning!"},{"content":"BentoML 基本流程：\ntrain.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。\nbentoml.(framename).save_model(modelname, model)\nframename取决于你构建机器学习模型时使用的框架。\nmodelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。\nmodel为你所构建的模型的变量名\n假如我此前已经定义了两个变量：\nmnist_clf = 'tensorflow_mnist'\nmnist_model = tf.keras.models.load_model('models/mnist_model.h5')\n那么有以下保存模型的例子：\nbentoml.tensorflow.save_model(mnist_clf, mnist_model)\ne.g. 基于sklearn实现的iris数据集模型：\nimport bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=\u0026#39;scale\u0026#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.sklearn.save_model(\u0026#34;iris_clf\u0026#34;, clf) print(f\u0026#34;Model saved: {saved_model}\u0026#34;) # Model saved: Model(tag=\u0026#34;iris_clf:zy3dfgxzqkjrlgxi\u0026#34;) 以下是BentoML目前支持的机器学习框架：\nCatBoost Diffusers fast.ai Keras LightGBM MLflow ONNX PyTorch PyTorch Lightning Scikit-Learn TensorFlow Transformers XGBoost Detectron2 EasyORC 二、创建服务 创建一个service.py文件以提供服务：\n# service.py import numpy as np from PIL.Image import Image as PILImage import bentoml from bentoml.io import Image from bentoml.io import NumpyNdarray # 实例化runner对象 mnist_runner = bentoml.tensorflow.get(\u0026#34;tensorflow_mnist:latest\u0026#34;).to_runner() # 创建服务 svc = bentoml.Service( name=\u0026#34;tensorflow_mnist_demo\u0026#34;, runners=[mnist_runner], ) # 提供预测服务的函数 @svc.api(input=Image(), output=NumpyNdarray(dtype=\u0026#34;float32\u0026#34;)) async def predict_image(f: PILImage) -\u0026gt; \u0026#34;np.ndarray\u0026#34;: assert isinstance(f, PILImage) arr = np.array(f) / 255.0 assert arr.shape == (28, 28) # We are using greyscale image and our PyTorch model expect one # extra channel dimension arr = np.expand_dims(arr, (0, 3)).astype(\u0026#34;float32\u0026#34;) # reshape to [1, 28, 28, 1] return await mnist_runner.async_run(arr) 现在，我们有了一个可以对MNIST手写数字识别数据集中的图片进行预测的BentoML服务了。\n三、构建Bento 一旦定义了服务，我们就可以将模型和服务制作成bento，Bento是服务的发布格式，它是一个独立的归档文件，运行服务所需的所有源代码、模型文件和依赖关系规范。\n要构建Bento，首先要在项目目录中创建一个bentofile.yaml文件：\nservice: \u0026#34;service:svc\u0026#34; description: \u0026#34;file: ./README.md\u0026#34; labels: owner: bentoml-team stage: demo include: - \u0026#34;*.py\u0026#34; exclude: - \u0026#34;locustfile.py\u0026#34; python: lock_packages: false packages: - tensorflow - Pillow BentoML在bentofile.yaml中提供了大量构建选项，用于自定义Python依赖关系、cuda安装、docker镜像分发等等。更多有关bentofile.yaml选项的信息请点击构建Bentos。\n接下来就可以在包含service.py和bentofile.yaml文件的目录下构建bento了！\n运行bentoml build CLI 命令：\n$ bentoml build ██████╗ ███████╗███╗ ██╗████████╗ ██████╗ ███╗ ███╗██╗ ██╔══██╗██╔════╝████╗ ██║╚══██╔══╝██╔═══██╗████╗ ████║██║ ██████╔╝█████╗ ██╔██╗ ██║ ██║ ██║ ██║██╔████╔██║██║ ██╔══██╗██╔══╝ ██║╚██╗██║ ██║ ██║ ██║██║╚██╔╝██║██║ ██████╔╝███████╗██║ ╚████║ ██║ ╚██████╔╝██║ ╚═╝ ██║███████╗ ╚═════╝ ╚══════╝╚═╝ ╚═══╝ ╚═╝ ╚═════╝ ╚═╝ ╚═╝╚══════╝ Successfully built Bento(tag=\u0026#34;tensorflow_mnist_demo:n5g45ibme2efgedi\u0026#34;). Possible next steps: * Containerize your Bento with `bentoml containerize`: $ bentoml containerize tensorflow_mnist_demo:n5g45ibme2efgedi [or bentoml build --containerize] * Push to BentoCloud with `bentoml push`: $ bentoml push tensorflow_mnist_demo:n5g45ibme2efgedi [or bentoml build --push] 当然 你也可以指定需要构建的bento：\nbentoml build -f ./src/my_project_a/bento_fraud_detect.yaml ./src/\n和保存模型类似，新创建的bento也会自动生成唯一的版本标签。\n你可以使用bentoml list查看你在本地构建的所有bento\n当你有多个bento的时候，你是否需要清理一些不需要的bento？\n你可以使用bentoml delete {bentoname:version}来删除不需要的bento\n四、进行预测 1、提供服务 目前已知的有两种提供服务的方式：\n本地使用命令行提供服务 首先使用bentoml serve {bentoname:version} CLI命令来运行它，得到一个bentoserver：\n此时我们向localhost:3000/{svc.api_name}发送请求时，携带我们需要让其进行预测的数据以及数据类型，便可以获得相应的预测结果了。\n将模型推送到Yatai上并部署 在bento build完成之后，可以将模型push到yatai上，在Yatai上根据提示部署模型。设置相应的服务器端口便可以使用预测服务了。\n2、进行预测 目前已知的有三种进行预测的方式：\n不使用服务，直接加载本地使用bentoml.{framename}.save_model方式构建出来的模型，再进行预测（目前eb使用的就是这种方式 import bentoml tensorflow_mnist_runner = bentoml.tensorflow.get(\u0026#34;tensorflow_mnist:latest\u0026#34;).to_runner() tensorflow_mnist_runner.init_local() tensorflow_mnist_runner.run(image) 使用服务，发送请求，通过服务进行预测从而获取结果 import requests requests_url = \u0026#34;http://127.0.0.1:3000/predict_image\u0026#34; def predict(img_url, request_url): # 获取二进制的url with open(img_url, \u0026#39;rb\u0026#39;) as f: img_bytes = f.read() # 向创建的服务发送预测请求 包含 请求地址、请求头（内容类型）、以及预测的数据 result = requests.post( \u0026#34;http://127.0.0.1:3000/predict_image\u0026#34;, headers={\u0026#34;content-type\u0026#34;: \u0026#34;image/png\u0026#34;}, data=img_bytes, ).text # 将预测结果转化为对应标签 result = eval(result)[0] max_value = max(result) max_index = result.index(max_value) # 返回预测出来的标签 return max_index results = [] for i in range(10): result = predict(f\u0026#39;./samples/{i}.png\u0026#39;, requests_url) results.append(result) print(results) \u0026gt;\u0026gt; output:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 使用Swagger UI，交互式的发送预测请求 点击Try it out:\n选择图片发送请求：\n查看返回结果：\n五、Swagger UI 基本流程如下：\n在构建Bento的时候在bento_name/version/apis/文件夹下生成了openapi.yaml文件，生成的openapi.yaml文件它遵循以下规范。 当使用bentoml serve在本地启动了BentoML服务或者在Yatai上部署后，它便会将openapi.yaml文档自动提供给Swagger UI，使其可以在前端渲染。 其中的docs.json是由Swagger UI从BentoML服务的OpenAPI规范文件自动生成的。 Swagger UI 中自动生成的内容\nsrc/bentoml/_internal/service/openapi/__init__.py\nopenapi.yaml文件生成的代码路径\nsrc/bentoml/_internal/bento/bento.py\n基于__init__.py中的函数openapi_spec构建\n定义了服务上传多个文件输入/输出的 API 规范\nsrc/bentoml/_internal/io_descriptors/multipart.py\n定义端点和前端内容\nsrc/bentoml/_internal/server/http_app.py\n","permalink":"https://blog.nyaashino.com/post/bentoml_quickstart/","summary":"BentoML 基本流程：\ntrain.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。\nbentoml.(framename).save_model(modelname, model)\nframename取决于你构建机器学习模型时使用的框架。\nmodelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。\nmodel为你所构建的模型的变量名\n假如我此前已经定义了两个变量：\nmnist_clf = 'tensorflow_mnist'\nmnist_model = tf.keras.models.load_model('models/mnist_model.h5')\n那么有以下保存模型的例子：\nbentoml.tensorflow.save_model(mnist_clf, mnist_model)\ne.g. 基于sklearn实现的iris数据集模型：\nimport bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=\u0026#39;scale\u0026#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.","title":"BentoML使用流程"}]