<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>hugo-PaperMod</title>
    <link>https://ACAne0320.github.io/</link>
    <description>Recent content on hugo-PaperMod</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 28 Sep 2024 18:58:25 +0800</lastBuildDate><atom:link href="https://ACAne0320.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Do it for youself</title>
      <link>https://ACAne0320.github.io/post/2024_review/</link>
      <pubDate>Sat, 28 Sep 2024 18:58:25 +0800</pubDate>
      
      <guid>https://ACAne0320.github.io/post/2024_review/</guid>
      <description>前言 不知不觉2024年也过去了一大半了，公司的各种变动让我丧失了对工作的热情，学不到新的技术，没有新的需求，要做的仅仅是运营简单的机器学习项目，我时常思考这样的工作能为我带来什么？我的领导对我很好，我也很感谢他对我的培养，但是……
鼓起勇气 在听闻需要调整薪资时，我脑袋里一片空白，之前想着公司在我最困难的时候愿意让我进去学习，而我能为公司做些什么呢？能陪伴他多久呢？现在看来，对于我本就不高的工资，在压力巨大的今天，抛去房租等日常开销，都不能随心所欲的想吃什么就吃什么了。 承诺的一年调薪两次也并没有做到，甚至还要降薪。好像是我不能接受的，我思来想去。
在收到薪资调整意向的一个月之后，我才下定决心找下一份工作。
看不到技术进步的工作、一趟一个半小时的通勤、需要调整的薪资，是压死骆驼的最后一根稻草。
努力……会有回报吗？ 我不知道。
下班的路上，同事和我说：
“我们是在这里干了这么久了，也不太好找下家。你不一样，趁着你还年轻，去拼一下吧。”
是啊，还年轻，是时候拼一下了。
第一次面试网易外包给我的打击太大了，我发现我好像什么都不会。于是这一个月以来，七点起床，一两点才睡觉，每天都很疲惫，下班后修改简历、背八股文、看书了解Python高级用法(Fluent Python)、了解k8s/Docker等等……
几乎没有其他额外开销的我，居然也攒不下什么钱，勉强够下一个月的房租。 我甚至怀疑是不是我的水平只够一份月薪7500的工作？噢不……甚至还要降。
在面试了几家之后，我觉得我的水平对应的薪资远不止如此，我觉得我可以有更大的天地……如果我面试不紧张能把脑子里的知识都好好表达出来的话。
勇敢迈向未来 我开始疯狂投简历找面试的机会，我想给我自己/给我爱的人更好的生活，累点苦点也无所谓。
我从未想过在工作了这么久之后，还是在天天吃泡面，不敢多花钱，更别说攒下一笔钱了。
既然选择了，就努力的去实现它，无论结果如何。
过程永远都会让你切实地感觉到你自己的进步。
Truth the Process.
结语 希望在你迷茫的时候，找到一个目标，无论目标有多难，首先得先去做。在执行的过程中，一步一步接近自己的目标，回头来看，你会感谢那个勇于尝试的自己。</description>
    </item>
    
    <item>
      <title>LLaMA-Factory Quickstart</title>
      <link>https://ACAne0320.github.io/post/llamafactory_quickstart/</link>
      <pubDate>Mon, 01 Apr 2024 17:43:46 +0800</pubDate>
      
      <guid>https://ACAne0320.github.io/post/llamafactory_quickstart/</guid>
      <description>项目github地址：https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file
项目hugging face地址：
一、环境构建 1、基础虚拟环境构建 git clone https://github.com/hiyouga/LLaMA-Factory.git conda create -n llama_factory python=3.10 conda activate llama_factory cd LLaMA-Factory pip install -r requirements.txt 如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 bitsandbytes 库, 支持 CUDA 11.1 到 12.2, 请根据您的 CUDA 版本情况选择适合的发布版本。
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl 2、在虚拟环境中安装cuda、cudnn 在新建的虚拟环境中使用conda安装cuda和cudnn。
conda install cudatoolkit cudnn 安装完成后，进入python环境进行验证，检测pytorch中CUDA是否能正常使用。
import torch torch.cuda.is_available() 如果返回True，则cuda安装正常。
如果返回False，请检查cuda和pytorch版本是否符合。
使用步骤1中的requirements.txt安装时，可能安装的是cpu版本的pytorch，使用conda list命令查看安装的pytorch包，如果是cpu版，则需要重新安装对应cuda版本的gpu版pytorch。
进入虚拟环境中执行以下命令，安装gpu版本的pytorch，下述演示的命令安装的为适配cuda11.8版本的pytorch，请按需选择版本。
pip3 install numpy --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118 二、启动LLaMA Board GUI 1、设置环境变量 CUDA_VISIBLE_DEVICES=0 python src/train_web.py 在Windows系统下，请设置系统环境变量CUDA_VISIBLE_DEVICES的值为0。</description>
    </item>
    
    <item>
      <title>New Beginning!</title>
      <link>https://ACAne0320.github.io/post/new_begining/</link>
      <pubDate>Sat, 28 Oct 2023 22:04:37 +0800</pubDate>
      
      <guid>https://ACAne0320.github.io/post/new_begining/</guid>
      <description>既然都建好自己的博客了，总得记录些什么吧o.0，于是乎……
今天是一个值得纪念的日子，为什么捏？请看~
一、工作 从入职到今天正好四个月，也顺利度过了试用期，工资也勉强够生活（虽然除去必要的开销之后所剩无几了，而且还需要还前几个月一点收入的没有的时候的债，在此特别感谢愿意借我钱的好哥们，虽然你们不一定能读到这篇文章，虽然我还钱还的很慢，但是我会努力的QAQ）。工作氛围也好，压力也没那么大，很幸运遇到这样的公司。
二、生活 今天搬进新房子啦，是一个一室一厅，虽然小了点，但我还是很喜欢的，租过程很愉快，中介也好，房东也好，都是很有爱的人。可能我的好运都用在这些地方了吧，从得到这份工作开始，最近也很少抱怨生活了，慢慢开始变得积极向上了。很感谢当时对我提供帮助的各位老师们，希望我的表现并没有让你们失望（虽然也不一定能读到这篇文章，我要点名表扬农老师！虽然我当面一直说不出那句感谢😄）。
三、新的开始 成功转正了，也租了属于自己的一间房子，真正出来工作之后才知道原来生活这么困难，光是供自己活下去都费尽全力了。 所以现在我很敬佩那些靠自己的能力努力活到现在并且保持乐观的人，我也会努力的向他们看齐。
插个今天搬家的小故事：
今天搬家的时候叫了一个货拉拉的小面包车，司机也是一个努力、乐观又可爱的人，和我一样也有点内向，但是我能听出来他的很多话都是发自内心的，可能这就是同性相吸吧。出小区的时候，由于停车超过了半个小时，要付停车费，我说我转给你吧，他说不用了，大家都挺不容易的。那一瞬间我有点感动，但也只是笑了笑说这样不好吧，直到最后他也没收我的钱。我也只能回几声感谢，一想到可能再也没有缘分再见了，有点遗憾呢。但是人生就是这样在一段又一段遗憾的中度过的，谁又能一帆风顺呢？
Keep calm and carry on!
虽然生活很难，但我们还是要继续前行。
努力变成乐观又元气满满的样子，谁能想到现在这样的我，在一年以前，是一个门都不敢出的玉玉症+自闭症+社恐+回避型依恋的人呢？很感谢过年的时候一直思考着是否要出去打拼、奋斗的自己，未来的你向你报喜啦，你成功了！虽然现在也很自闭，但是相比以前，已经好太多了，我能看见我自己的进步，所以我并不觉得我比别人差。
最近加上午休每天也只睡了大概七个小时，睡眠质量极差。
希望搬到新的环境中会有所改善吧🙏
4、END 最后送大家正好听到的一首歌的一段歌词：
只能由我自己来踏出这一步，不能在水边傻傻等待明天到来。</description>
    </item>
    
    <item>
      <title>BentoML使用流程</title>
      <link>https://ACAne0320.github.io/post/bentoml_quickstart/</link>
      <pubDate>Thu, 26 Oct 2023 17:20:02 +0800</pubDate>
      
      <guid>https://ACAne0320.github.io/post/bentoml_quickstart/</guid>
      <description>BentoML 基本流程：
train.py训练模型 使用bentoml保存模型 构建service.py，创建相应的服务 创建bentofile.yaml文件 使用命令行bentoml build 构建Bento 将构建好的Bento推送到Yatai上部署 / 在本地使用命令启动服务 发送预测请求 一、使用BentoML保存模型 如果想要开始使用BentoML服务，首先需要将训练的模型使用BentoML的API在本地进行保存。
bentoml.(framename).save_model(modelname, model)
framename取决于你构建机器学习模型时使用的框架。
modelname为模型保存后的名字，并且会自动生成一个版本字段，用于检索模型。
model为你所构建的模型的变量名
假如我此前已经定义了两个变量：
mnist_clf = &#39;tensorflow_mnist&#39;
mnist_model = tf.keras.models.load_model(&#39;models/mnist_model.h5&#39;)
那么有以下保存模型的例子：
bentoml.tensorflow.save_model(mnist_clf, mnist_model)
e.g. 基于sklearn实现的iris数据集模型：
import bentoml from sklearn import svm from sklearn import datasets # Load training data set iris = datasets.load_iris() X, y = iris.data, iris.target # Train the model clf = svm.SVC(gamma=&amp;#39;scale&amp;#39;) clf.fit(X, y) # Save model to the BentoML local model store saved_model = bentoml.</description>
    </item>
    
    <item>
      <title>A regular player, listener, developer.</title>
      <link>https://ACAne0320.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ACAne0320.github.io/about/</guid>
      <description>永远是深夜有多好。</description>
    </item>
    
    
    
  </channel>
</rss>
